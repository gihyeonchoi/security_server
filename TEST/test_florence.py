from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
import torch
import torch.nn.functional as F
import numpy as np
import re
import warnings

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
print("Florence-2 ëª¨ë¸ ë¡œë”© ì¤‘...")
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Florence-2-base",
    trust_remote_code=True,
    attn_implementation="eager"
)

processor = AutoProcessor.from_pretrained(
    "microsoft/Florence-2-base", 
    trust_remote_code=True
)

# GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
print(f"ë””ë°”ì´ìŠ¤: {device}")

# ì´ë¯¸ì§€ ë¡œë“œ
image_path = "photo/fight.jpg"
try:
    image = Image.open(image_path)
    print(f"\nì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ: {image_path}")
    print(f"ì´ë¯¸ì§€ í¬ê¸°: {image.size}")
except FileNotFoundError:
    print(f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    exit()

# 4ê°€ì§€ ìƒí™© ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
scenarios = [
    "ë‘ ë‚¨ìê°€ ì‹¸ìš°ëŠ” ìƒí™©",
    "ë‘ ì‚¬ëŒì´ ìŒì‹ì„ ë¨¹ëŠ” ìƒí™©", 
    "ë‘ ì‚¬ëŒì´ ëŒ€í™”í•˜ëŠ” ìƒí™©",
    "ë‘ ì‚¬ëŒì´ ìš´ë™í•˜ëŠ” ìƒí™©"
]

print("\n" + "="*50)
print("Florence-2ë¥¼ ì‚¬ìš©í•œ ìƒí™© ë¶„ì„")
print("="*50)

# 1. Caption ìƒì„± (post_process ì—†ì´ ì§ì ‘ íŒŒì‹±)
print("\nì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")

all_captions = []

# DETAILED_CAPTION ìƒì„±
task = "<DETAILED_CAPTION>"
inputs = processor(text=task, images=image, return_tensors="pt").to(device)

with torch.no_grad():
    generated_ids = model.generate(
        input_ids=inputs["input_ids"],
        pixel_values=inputs["pixel_values"],
        max_new_tokens=200,
        num_beams=3,
        do_sample=False
    )
    
    # ë””ì½”ë”©
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    
    # íƒœìŠ¤í¬ í† í° ì œê±°í•˜ê³  ì‹¤ì œ caption ì¶”ì¶œ
    # Florence-2ëŠ” ë³´í†µ "task_token<ì‹¤ì œë‚´ìš©>" í˜•íƒœë¡œ ì¶œë ¥
    caption = generated_text.replace(task, "").strip()
    
    # ì¶”ê°€ í† í° ì œê±°
    caption = re.sub(r'<[^>]+>', '', caption).strip()
    
    all_captions.append(caption)
    print(f"\nì‹¤ì œ ì´ë¯¸ì§€ ì„¤ëª…:")
    print(f"'{caption}'")

# CAPTION (ê°„ë‹¨) ìƒì„±
task = "<CAPTION>"
inputs = processor(text=task, images=image, return_tensors="pt").to(device)

with torch.no_grad():
    generated_ids = model.generate(
        input_ids=inputs["input_ids"],
        pixel_values=inputs["pixel_values"],
        max_new_tokens=50,
        num_beams=3,
        do_sample=False
    )
    
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    simple_caption = generated_text.replace(task, "").strip()
    simple_caption = re.sub(r'<[^>]+>', '', simple_caption).strip()
    all_captions.append(simple_caption)

# ëª¨ë“  captionì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
combined_caption = ' '.join(all_captions).lower()

print("\n" + "-"*50)

# 2. ê° ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ í‚¤ì›Œë“œ ì •ì˜ ë° ë§¤ì¹­
scenario_keywords = {
    "ë‘ ë‚¨ìê°€ ì‹¸ìš°ëŠ” ìƒí™©": {
        "strong": ["fight", "fighting", "punch", "kick", "combat", "boxing", "martial", "wrestl"],
        "medium": ["aggressive", "conflict", "confrontation", "attack", "violent", "battle"],
        "weak": ["angry", "tension", "dispute", "physical", "fist", "stance"]
    },
    "ë‘ ì‚¬ëŒì´ ìŒì‹ì„ ë¨¹ëŠ” ìƒí™©": {
        "strong": ["eating", "food", "meal", "dining", "restaurant", "lunch", "dinner"],
        "medium": ["table", "plate", "fork", "spoon", "drink", "cuisine", "dish"],
        "weak": ["kitchen", "cafe", "breakfast", "snack", "beverage", "cook"]
    },
    "ë‘ ì‚¬ëŒì´ ëŒ€í™”í•˜ëŠ” ìƒí™©": {
        "strong": ["talking", "conversation", "speaking", "discussing", "chat", "dialogue"],
        "medium": ["communication", "meeting", "interview", "discussion", "talk", "speech"],
        "weak": ["face", "gesture", "listening", "explaining", "mouth", "words"]
    },
    "ë‘ ì‚¬ëŒì´ ìš´ë™í•˜ëŠ” ìƒí™©": {
        "strong": ["exercise", "workout", "sport", "training", "gym", "fitness"],
        "medium": ["athletic", "running", "lifting", "stretching", "sparring", "practice"],
        "weak": ["active", "physical", "sweat", "muscle", "movement", "activity"]
    }
}

# 3. ì ìˆ˜ ê³„ì‚°
print("\ní‚¤ì›Œë“œ ë§¤ì¹­ ë¶„ì„:")
print("-"*50)

scores = []
for scenario in scenarios:
    keywords = scenario_keywords[scenario]
    
    # ê°€ì¤‘ì¹˜ ì ìš© ì ìˆ˜ ê³„ì‚°
    strong_score = sum(3 for word in keywords["strong"] if word in combined_caption)
    medium_score = sum(2 for word in keywords["medium"] if word in combined_caption)
    weak_score = sum(1 for word in keywords["weak"] if word in combined_caption)
    
    total_score = strong_score + medium_score + weak_score
    scores.append(total_score)
    
    # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì¶œë ¥
    matched_keywords = []
    for level, words in keywords.items():
        for word in words:
            if word in combined_caption:
                matched_keywords.append(f"{word}({level[0]})")
    
    if matched_keywords:
        print(f"{scenario}:")
        print(f"  ë§¤ì¹­: {', '.join(matched_keywords[:5])}")  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ

# 4. ì ìˆ˜ê°€ ëª¨ë‘ 0ì¸ ê²½ìš° ì²˜ë¦¬
if all(score == 0 for score in scores):
    print("\ní‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨ - ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰")
    # ê¸°ë³¸ ë‹¨ì–´ ê²€ìƒ‰
    if "man" in combined_caption or "men" in combined_caption or "people" in combined_caption:
        if any(word in combined_caption for word in ["two", "2", "pair"]):
            scores[0] = 1  # ê¸°ë³¸ê°’ ë¶€ì—¬

# 5. Softmaxë¥¼ ì‚¬ìš©í•œ í™•ë¥  ë³€í™˜
if sum(scores) > 0:
    # ì˜¨ë„ ë§¤ê°œë³€ìˆ˜ë¡œ í™•ë¥  ë¶„í¬ ì¡°ì •
    temperature = 1.5
    scores_array = np.array(scores, dtype=float)
    # 0 ì œê±°ë¥¼ ìœ„í•´ ì‘ì€ ê°’ ì¶”ê°€
    scores_array = scores_array + 0.1
    exp_scores = np.exp(scores_array / temperature)
    probabilities = exp_scores / exp_scores.sum()
else:
    # ì ìˆ˜ê°€ ëª¨ë‘ 0ì¸ ê²½ìš° ê· ë“± ë¶„í¬
    probabilities = np.array([0.25] * 4)

# 6. ê²°ê³¼ ì¶œë ¥
print("\n" + "="*50)
print("ğŸ“Š ê° ìƒí™©ë³„ í™•ë¥ :")
print("="*50)

results = []
for scenario, prob in zip(scenarios, probabilities):
    results.append((scenario, prob))
    
    # ì‹œê°ì  ë°” ê·¸ë˜í”„
    bar_length = int(prob * 30)
    bar = 'â–ˆ' * bar_length + 'â–‘' * (30 - bar_length)
    
    # í™•ë¥ ì— ë”°ë¥¸ ìƒ‰ìƒ ì´ëª¨ì§€
    if prob > 0.5:
        emoji = "ğŸ”´"
    elif prob > 0.3:
        emoji = "ğŸŸ¡"
    else:
        emoji = "âšª"
    
    print(f"\n{emoji} {scenario}")
    print(f"   {bar} {prob:.4f} ({prob*100:.2f}%)")

# 7. ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ìƒí™©
print("\n" + "="*50)
best_scenario, best_prob = max(results, key=lambda x: x[1])
print(f"ğŸ¯ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ìƒí™©:")
print(f"   â†’ {best_scenario}")
print(f"   â†’ í™•ë¥ : {best_prob:.4f} ({best_prob*100:.2f}%)")

# 8. ì‹ ë¢°ë„ í‰ê°€
sorted_probs = sorted(probabilities, reverse=True)
if len(sorted_probs) > 1:
    confidence = sorted_probs[0] - sorted_probs[1]
    if confidence > 0.3:
        confidence_level = "ë†’ìŒ â­â­â­"
    elif confidence > 0.15:
        confidence_level = "ì¤‘ê°„ â­â­"
    else:
        confidence_level = "ë‚®ìŒ â­"
    
    print(f"\nğŸ“ˆ ì‹ ë¢°ë„: {confidence_level} (ì°¨ì´: {confidence:.4f})")

# 9. Object Detectionìœ¼ë¡œ ì¶”ê°€ ë¶„ì„
print("\n" + "="*50)
print("ğŸ” ì¶”ê°€ Object Detection ë¶„ì„:")
print("="*50)

task = "<OD>"
inputs = processor(text=task, images=image, return_tensors="pt").to(device)

with torch.no_grad():
    generated_ids = model.generate(
        input_ids=inputs["input_ids"],
        pixel_values=inputs["pixel_values"],
        max_new_tokens=1024,
        num_beams=3,
        do_sample=False
    )
    
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    
    # OD ê²°ê³¼ íŒŒì‹± (ìˆ˜ë™)
    od_result = generated_text.replace(task, "").strip()
    
    # ê°ì²´ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
    if od_result:
        # Florence-2 OD ì¶œë ¥ í˜•ì‹: "obj<loc>coords</loc>..."
        objects = re.findall(r'([a-zA-Z\s]+)<loc>', od_result)
        if objects:
            unique_objects = list(set([obj.strip() for obj in objects]))
            print(f"íƒì§€ëœ ê°ì²´ë“¤: {', '.join(unique_objects)}")
            
            # ì‚¬ëŒ ê´€ë ¨ ê°ì²´ í™•ì¸
            person_keywords = ['person', 'people', 'man', 'men', 'woman', 'women', 'human']
            person_count = sum(1 for obj in objects if any(keyword in obj.lower() for keyword in person_keywords))
            
            if person_count >= 2:
                print(f"  â†’ {person_count}ëª…ì˜ ì‚¬ëŒ íƒì§€ë¨")
            
            # íŠ¹ì • ê°ì²´ í™•ì¸
            if any('fight' in obj.lower() or 'combat' in obj.lower() for obj in objects):
                print("  â†’ ì‹¸ì›€ ê´€ë ¨ ë™ì‘ íƒì§€")
        else:
            print("  â†’ ê°ì²´ íŒŒì‹± ì‹¤íŒ¨ (í˜•ì‹ ë¶ˆì¼ì¹˜)")
    else:
        print("  â†’ Object Detection ê²°ê³¼ ì—†ìŒ")

print("\n" + "="*50)
print("âœ… ë¶„ì„ ì™„ë£Œ!")
print("="*50)

# 10. ìš”ì•½ ì •ë³´
print("\nğŸ“‹ ìš”ì•½:")
print(f"  â€¢ ì…ë ¥ ì´ë¯¸ì§€: {image_path}")
print(f"  â€¢ ì´ë¯¸ì§€ í¬ê¸°: {image.size}")
print(f"  â€¢ ìµœì¢… ì˜ˆì¸¡: {best_scenario}")
print(f"  â€¢ í™•ë¥ : {best_prob:.2%}")
print("="*50)