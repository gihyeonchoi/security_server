import torch
from PIL import Image
from transformers import BlipProcessor, BlipForImageTextRetrieval

class SimpleBLIPITM:
    def __init__(self):
        print("Loading BLIP ITM model...")
        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-itm-base-coco")
        self.model = BlipForImageTextRetrieval.from_pretrained(
            "Salesforce/blip-itm-base-coco",
            torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ì ˆì•½
            low_cpu_mem_usage=True
        )
        print("âœ… Model loaded successfully")
    
    def get_similarity_score(self, image, text):
        """ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (0~1)"""
        inputs = self.processor(image, text, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # ITM ì ìˆ˜ ì¶”ì¶œ (ì—¬ëŸ¬ í˜•íƒœ ì²˜ë¦¬)
        if hasattr(outputs, 'itm_score'):
            score_tensor = outputs.itm_score
        elif hasattr(outputs, 'logits'):
            score_tensor = outputs.logits
        else:
            # ì¶œë ¥ êµ¬ì¡° í™•ì¸
            print(f"Available outputs: {dir(outputs)}")
            score_tensor = outputs[0] if len(outputs) > 0 else torch.tensor([0.5])
        
        # í…ì„œ ì°¨ì› ì²˜ë¦¬
        if score_tensor.dim() > 0:
            if score_tensor.shape[-1] == 2:  # [negative, positive] í˜•íƒœ
                score = torch.softmax(score_tensor, dim=-1)[..., 1]  # positive ì ìˆ˜ë§Œ
            else:
                score = score_tensor.flatten()[0]  # ì²« ë²ˆì§¸ ê°’
        else:
            score = score_tensor
        
        # ì‹œê·¸ëª¨ì´ë“œ ì ìš©í•˜ì—¬ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        score = torch.sigmoid(score).item()
        return score
    
    def classify_image(self, image, candidate_texts):
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ ì°¾ê¸°"""
        scores = {}
        
        print(f"ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì¤‘... (í›„ë³´: {len(candidate_texts)}ê°œ)")
        
        for text in candidate_texts:
            score = self.get_similarity_score(image, text)
            scores[text] = score
            print(f"   '{text}': {score:.4f}")
        
        # ìµœê³  ì ìˆ˜ ì°¾ê¸°
        best_text = max(scores.keys(), key=lambda k: scores[k])
        best_score = scores[best_text]
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ìµœê³ ì ê³¼ ë‘ë²ˆì§¸ ì ìˆ˜ì˜ ì°¨ì´)
        sorted_scores = sorted(scores.values(), reverse=True)
        confidence_gap = sorted_scores[0] - sorted_scores[1] if len(sorted_scores) > 1 else sorted_scores[0]
        
        return {
            'prediction': best_text,
            'confidence': best_score,
            'confidence_gap': confidence_gap,
            'all_scores': scores,
            'is_confident': confidence_gap > 0.1  # ì°¨ì´ê°€ 0.1 ì´ìƒì´ë©´ í™•ì‹ 
        }

# ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def quick_test(image_path, candidate_texts=None):
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    
    if candidate_texts is None:
        candidate_texts = ['monitor', 'keyboard', 'speaker', 'mouse']
    
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(image_path).convert("RGB")
        print(f"ğŸ“· ì´ë¯¸ì§€ ë¡œë“œ: {image_path} ({image.size})")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        classifier = SimpleBLIPITM()
        
        # ë¶„ë¥˜ ì‹¤í–‰
        result = classifier.classify_image(image, candidate_texts)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ¯ ë¶„ë¥˜ ê²°ê³¼:")
        print(f"   ì˜ˆì¸¡: {result['prediction']}")
        print(f"   ì ìˆ˜: {result['confidence']:.4f}")
        print(f"   ì‹ ë¢°ë„ ì°¨ì´: {result['confidence_gap']:.4f}")
        print(f"   í™•ì‹  ì—¬ë¶€: {'âœ… í™•ì‹ í•¨' if result['is_confident'] else 'â“ ë¶ˆí™•ì‹¤'}")
        
        print("\nğŸ“Š ëª¨ë“  ì ìˆ˜:")
        for text, score in result['all_scores'].items():
            status = "ğŸ‘‘" if text == result['prediction'] else "  "
            print(f"   {status} {text}: {score:.4f}")
        
        return result
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return None

# ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
def test_multiple_images(image_paths, candidate_texts):
    """ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ í•œë²ˆì— í…ŒìŠ¤íŠ¸"""
    
    classifier = SimpleBLIPITM()
    results = []
    
    for i, image_path in enumerate(image_paths):
        print(f"\nğŸ” ì´ë¯¸ì§€ {i+1}/{len(image_paths)}: {image_path}")
        
        try:
            image = Image.open(image_path).convert("RGB")
            result = classifier.classify_image(image, candidate_texts)
            result['image_path'] = image_path
            results.append(result)
            
            print(f"   ê²°ê³¼: {result['prediction']} ({result['confidence']:.3f})")
            
        except Exception as e:
            print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            results.append({'image_path': image_path, 'error': str(e)})
    
    return results

# ì„ê³„ê°’ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_with_threshold(image_path, candidate_texts, threshold=0.5):
    """ì„ê³„ê°’ì„ ì‚¬ìš©í•œ ë¶„ë¥˜"""
    
    classifier = SimpleBLIPITM()
    image = Image.open(image_path).convert("RGB")
    
    print(f"ğŸ” ì„ê³„ê°’ í…ŒìŠ¤íŠ¸ (threshold: {threshold})")
    
    scores = {}
    for text in candidate_texts:
        score = classifier.get_similarity_score(image, text)
        scores[text] = score
        
        status = "âœ… PASS" if score > threshold else "âŒ FAIL"
        print(f"   '{text}': {score:.4f} {status}")
    
    # ì„ê³„ê°’ ì´ìƒì¸ í•­ëª©ë“¤ë§Œ í•„í„°ë§
    passed_items = {k: v for k, v in scores.items() if v > threshold}
    
    if passed_items:
        best_item = max(passed_items.keys(), key=lambda k: passed_items[k])
        print(f"\nğŸ¯ ì„ê³„ê°’ í†µê³¼: {best_item} ({passed_items[best_item]:.4f})")
    else:
        print(f"\nâŒ ì„ê³„ê°’ í†µê³¼ í•­ëª© ì—†ìŒ (ìµœê³ ì : {max(scores.values()):.4f})")
    
    return scores

# ë””ë²„ê¹… í•¨ìˆ˜ ì¶”ê°€
def debug_model_output(image_path, text="monitor"):
    """ëª¨ë¸ ì¶œë ¥ êµ¬ì¡° í™•ì¸"""
    from transformers import BlipProcessor, BlipForImageTextRetrieval
    
    print("ğŸ”§ ëª¨ë¸ ì¶œë ¥ ë””ë²„ê¹…...")
    
    processor = BlipProcessor.from_pretrained("Salesforce/blip-itm-base-coco")
    model = BlipForImageTextRetrieval.from_pretrained("Salesforce/blip-itm-base-coco")
    
    image = Image.open(image_path).convert("RGB")
    inputs = processor(image, text, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    print(f"ì¶œë ¥ íƒ€ì…: {type(outputs)}")
    print(f"ì¶œë ¥ ì†ì„±: {dir(outputs)}")
    
    if hasattr(outputs, 'itm_score'):
        print(f"itm_score í˜•íƒœ: {outputs.itm_score.shape}")
        print(f"itm_score ê°’: {outputs.itm_score}")
    
    if hasattr(outputs, 'logits'):
        print(f"logits í˜•íƒœ: {outputs.logits.shape}")
        print(f"logits ê°’: {outputs.logits}")
    
    return outputs


print("ğŸ§ª BLIP ITM ê°„ë‹¨ í…ŒìŠ¤íŠ¸")
print("="*50)

# ì‚¬ìš© ì˜ˆì‹œ
# print("ì‚¬ìš©ë²•:")
# print("1. ë‹¨ì¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸:")
# print("   result = quick_test('your_image.jpg')")
# print()
# print("2. ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸:")
# print("   result = quick_test('image.jpg', ['computer', 'phone', 'tablet'])")
# print()
# print("3. ì—¬ëŸ¬ ì´ë¯¸ì§€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸:")
# print("   results = test_multiple_images(['img1.jpg', 'img2.jpg'], ['monitor', 'keyboard'])")
# print()
# print("4. ì„ê³„ê°’ í…ŒìŠ¤íŠ¸:")
# print("   scores = test_with_threshold('image.jpg', ['monitor', 'keyboard'], 0.6)")
# print()
# print("ì„¤ì¹˜ ëª…ë ¹:")
# print("pip install transformers torch pillow")

# ì‹¤ì œ í…ŒìŠ¤íŠ¸ (ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½)
result = quick_test("clip_images/tmpeb_qopzg.PNG")