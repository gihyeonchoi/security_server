# CCTV/utils.py
import cv2
import threading
import time
import queue
import os
from django.http import StreamingHttpResponse
from django.conf import settings
import json
import numpy as np
from collections import deque
from ultralytics import YOLO
import torch
from PIL import Image, ImageDraw, ImageFont
import clip
import threading
from datetime import datetime

# ì „ì—­ ì•Œë¦¼ í (ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ê°€ ê³µìœ )
GLOBAL_ALERT_QUEUE = queue.Queue(maxsize=100)
ALERT_LISTENERS = []  # SSE ë¦¬ìŠ¤ë„ˆë“¤ì„ ì €ì¥

class CameraStreamer:
    def __init__(self):
        self.cameras = {}
        self.global_lock = threading.Lock()
        self.active_streams = {}
        self.frame_queues = {}
        self.reader_threads = {}
        self.background_streaming = {}  # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ì¶”ì 
    
    def refresh_cameras(self):
        """ì¹´ë©”ë¼ ëª©ë¡ ë³€ê²½ ê°ì§€ í›„ ìŠ¤íŠ¸ë¦¬ë°ì„ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            from .models import Camera
            
            print("ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ ì¹´ë©”ë¼ ëª©ë¡ ì—…ë°ì´íŠ¸")
            
            # í˜„ì¬ DBì˜ ì¹´ë©”ë¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            current_cameras = Camera.objects.all()
            current_rtsp_urls = set(camera.rtsp_url for camera in current_cameras)
            
            # í˜„ì¬ í™œì„±í™”ëœ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ëª©ë¡
            active_background = set(self.background_streaming.keys()) if self.background_streaming else set()
            
            # 1. ì‚­ì œëœ ì¹´ë©”ë¼ë“¤ì˜ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€ (ì•ˆì „í•˜ê²Œ)
            removed_urls = active_background - current_rtsp_urls
            for rtsp_url in removed_urls:
                try:
                    print(f"â¹ï¸ ì‚­ì œëœ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€: {rtsp_url}")
                    self.stop_background_streaming(rtsp_url)
                    # ì§ì ‘ ì •ë¦¬í•˜ì§€ ë§ê³  ë‹¤ìŒ ì‚¬ì´í´ì—ì„œ ìë™ ì •ë¦¬ë˜ë„ë¡ ë‚¨ê²¨ë‘¡ì‹œ
                    # self.cleanup_camera(rtsp_url)  # ì´ ë¼ì¸ì„ ì£¼ì„ ì²˜ë¦¬
                except Exception as e:
                    print(f"âš ï¸ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€ ì˜¤ë¥˜: {e}")
            
            # 2. ìƒˆë¡œ ì¶”ê°€ëœ ì¹´ë©”ë¼ë“¤ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
            new_urls = current_rtsp_urls - active_background
            for camera in current_cameras:
                if camera.rtsp_url in new_urls:
                    try:
                        print(f"ğŸš€ ìƒˆ ì¹´ë©”ë¼ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {camera.name}")
                        self.start_background_streaming(camera.rtsp_url)
                    except Exception as e:
                        print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì‹¤íŒ¨: {e}")
            
            print("âœ… ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    def get_camera_stream(self, rtsp_url):
        # print(f"ğŸ” global_lock íšë“ ì‹œë„: {rtsp_url}")
        
        # ë½ íšë“ ì‹œë„ (íƒ€ì„ì•„ì›ƒ 5ì´ˆ)
        lock_acquired = self.global_lock.acquire(timeout=5.0)
        if not lock_acquired:
            # print(f"âŒ global_lock íšë“ ì‹¤íŒ¨ (íƒ€ì„ì•„ì›ƒ): {rtsp_url}")
            pass
            # ì„ì‹œë¡œ ê¸°ë³¸ ì¹´ë©”ë¼ ì •ë³´ ë°˜í™˜
            if rtsp_url not in self.cameras:
                self.cameras[rtsp_url] = {
                    'cap': None,
                    'is_connected': False,
                    'last_frame': None,
                    'fps_counter': 0,
                    'last_fps_time': time.time(),
                    'avg_fps': 0,
                    'tracker_count': 0,
                    'lock': threading.Lock(),
                    'stream_count': 0,
                    'reconnect_attempts': 0,
                    'last_reconnect_time': 0
                }
                self.frame_queues[rtsp_url] = queue.Queue(maxsize=5)
            return self.cameras[rtsp_url]
        
        try:
            # print(f"âœ… global_lock íšë“ ì„±ê³µ: {rtsp_url}")
            if rtsp_url not in self.cameras:
                self.cameras[rtsp_url] = {
                    'cap': None,
                    'is_connected': False,
                    'last_frame': None,
                    'fps_counter': 0,
                    'last_fps_time': time.time(),
                    'avg_fps': 0,
                    'tracker_count': 0,
                    'lock': threading.Lock(),
                    'stream_count': 0,
                    'reconnect_attempts': 0,
                    'last_reconnect_time': 0
                }
                # ê° ì¹´ë©”ë¼ë³„ í”„ë ˆì„ í ìƒì„± (ë°±ê·¸ë¼ìš´ë“œ ëª¨ë“œë¥¼ ìœ„í•´ ì•½ê°„ ë” í° í)
                self.frame_queues[rtsp_url] = queue.Queue(maxsize=5)
            return self.cameras[rtsp_url]
        finally:
            self.global_lock.release()
            # print(f"ğŸ”“ global_lock í•´ì œ: {rtsp_url}")
    
    def connect_camera(self, rtsp_url):
        """ì¹´ë©”ë¼ ì—°ê²° - ë²„í¼ë§ ìµœì†Œí™” ë²„ì „"""
        camera_info = self.get_camera_stream(rtsp_url)
        
        with camera_info['lock']:
            # ì¬ì—°ê²° ì‹œë„ ì œí•œ
            current_time = time.time()
            if camera_info['reconnect_attempts'] >= 3:
                if current_time - camera_info['last_reconnect_time'] < 30:
                    return False
                else:
                    camera_info['reconnect_attempts'] = 0
            
            if camera_info['cap'] is None or not camera_info['is_connected']:
                try:
                    if camera_info['cap']:
                        camera_info['cap'].release()
                        time.sleep(0.1)
                    
                    # FFmpeg ë°±ì—”ë“œ ì‚¬ìš© (ì•ˆì •ì„± í–¥ìƒ)
                    backend = cv2.CAP_FFMPEG
                    
                    # RTSP URLì— íŒŒë¼ë¯¸í„° ì¶”ê°€ (ë‚®ì€ ì§€ì—°ì‹œê°„ ë° ì•ˆì •ì„±)
                    rtsp_url_optimized = rtsp_url
                    if '?' not in rtsp_url:
                        # TCP ì‚¬ìš© + ì¶”ê°€ ì•ˆì •ì„± ì˜µì…˜
                        rtsp_url_optimized = f"{rtsp_url}?tcp&timeout=5000000&stimeout=5000000"
                    
                    print(f"ğŸ”— RTSP ì—°ê²° ì‹œë„: {rtsp_url_optimized}")
                    cap = cv2.VideoCapture(rtsp_url_optimized, backend)
                    
                    # ë²„í¼ í¬ê¸° ìµœì†Œí™” (ë©€í‹° ìŠ¤íŠ¸ë¦¼ ì•ˆì •ì„± í–¥ìƒ)
                    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                    
                    # FFmpeg ì•ˆì •ì„± ì˜µì…˜
                    try:
                        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'H264'))
                        cap.set(cv2.CAP_PROP_FPS, 25)
                        
                        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì§§ê²Œ ì„¤ì •í•˜ì—¬ ë¹ ë¥¸ ì‹¤íŒ¨ ê°ì§€)
                        cap.set(cv2.CAP_PROP_OPEN_TIMEOUT_MSEC, 3000)
                        cap.set(cv2.CAP_PROP_READ_TIMEOUT_MSEC, 3000)
                        
                        # FFmpeg ìŠ¤ë ˆë“œ ì•ˆì •ì„± ì˜µì…˜
                        if hasattr(cv2, 'CAP_PROP_FRAME_MSEC'):
                            cap.set(cv2.CAP_PROP_FRAME_MSEC, 40)  # 25 FPS = 40ms
                            
                    except Exception as prop_error:
                        print(f"âš ï¸ ì¹´ë©”ë¼ ì†ì„± ì„¤ì • ì˜¤ë¥˜: {prop_error}")
                    
                    if cap.isOpened():
                        # ë²„í¼ ë¹„ìš°ê¸° - ìµœì‹  í”„ë ˆì„ê¹Œì§€ ìŠ¤í‚µ
                        print(f"ğŸ”„ ë²„í¼ ë¹„ìš°ê¸° ì‹œì‘: {rtsp_url}")
                        flush_start = time.time()
                        frames_flushed = 0
                        
                        # ìµœëŒ€ 2ì´ˆ ë™ì•ˆ ë²„í¼ ë¹„ìš°ê¸°
                        while time.time() - flush_start < 2.0:
                            ret = cap.grab()  # grab()ì€ read()ë³´ë‹¤ ë¹ ë¦„
                            if not ret:
                                break
                            frames_flushed += 1
                            
                            # 30í”„ë ˆì„ë§ˆë‹¤ ì‹¤ì œ ì½ê¸° í…ŒìŠ¤íŠ¸
                            if frames_flushed % 30 == 0:
                                ret, test_frame = cap.retrieve()
                                if not ret or test_frame is None:
                                    break
                        
                        print(f"âœ… ë²„í¼ ë¹„ìš°ê¸° ì™„ë£Œ: {frames_flushed}ê°œ í”„ë ˆì„ ìŠ¤í‚µ")
                        
                        # ìµœì‹  í”„ë ˆì„ í…ŒìŠ¤íŠ¸
                        ret, test_frame = cap.read()
                        if ret and test_frame is not None:
                            camera_info['cap'] = cap
                            camera_info['is_connected'] = True
                            camera_info['reconnect_attempts'] = 0
                            
                            print(f"âœ… ì¹´ë©”ë¼ ì—°ê²° ì„±ê³µ: {rtsp_url}")
                            
                            # ê¸°ì¡´ ìŠ¤ë ˆë“œê°€ ì‚´ì•„ìˆëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸
                            old_thread = self.reader_threads.get(rtsp_url)
                            if old_thread and old_thread.is_alive():
                                print(f"âš ï¸ ê¸°ì¡´ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°: {rtsp_url}")
                                try:
                                    old_thread.join(timeout=1.0)
                                except:
                                    pass
                            
                            # ìƒˆ í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ ì‹œì‘
                            if rtsp_url not in self.reader_threads or not self.reader_threads[rtsp_url].is_alive():
                                reader_thread = threading.Thread(
                                    target=self._frame_reader_thread_optimized,
                                    args=(rtsp_url,),
                                    daemon=True,
                                    name=f"FrameReader-{rtsp_url.split('/')[-1][:10]}"
                                )
                                self.reader_threads[rtsp_url] = reader_thread
                                reader_thread.start()
                                print(f"ğŸš€ í”„ë ˆì„ ë¦¬ë” ìŠ¤ë ˆë“œ ì‹œì‘: {reader_thread.name}")
                            
                            return True
                        else:
                            cap.release()
                            camera_info['reconnect_attempts'] += 1
                            camera_info['last_reconnect_time'] = current_time
                            return False
                    else:
                        camera_info['reconnect_attempts'] += 1
                        camera_info['last_reconnect_time'] = current_time
                        return False
                except Exception as e:
                    print(f"Camera connection error: {e}")
                    if 'cap' in locals() and cap:
                        cap.release()
                    camera_info['reconnect_attempts'] += 1
                    camera_info['last_reconnect_time'] = current_time
                    return False
            return camera_info['is_connected']
    
    def _frame_reader_thread_optimized(self, rtsp_url):
        """í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ - FFmpeg ì•ˆì •ì„± ê°•í™” ë²„ì „"""
        thread_name = threading.current_thread().name
        print(f"ğŸ“º í”„ë ˆì„ ë¦¬ë” ì‹œì‘: {thread_name} ({rtsp_url})")
        
        camera_info = self.cameras.get(rtsp_url)
        frame_queue = self.frame_queues.get(rtsp_url)
        
        if not camera_info or not frame_queue:
            print(f"âŒ ì¹´ë©”ë¼ ì •ë³´ ë˜ëŠ” íê°€ ì—†ìŒ: {rtsp_url}")
            return
        
        consecutive_failures = 0
        last_frame_time = time.time()
        frame_skip_counter = 0
        last_error_log = 0  # ì˜¤ë¥˜ ë¡œê·¸ ë¹ˆë„ ì œí•œ
        current_time = time.time()  # ë³€ìˆ˜ ì´ˆê¸°í™” ì¶”ê°€
        
        try:
            while True:
                # ì¹´ë©”ë¼ ìƒíƒœ í™•ì¸ (ì•ˆì „í•˜ê²Œ)
                try:
                    with camera_info['lock']:
                        cap = camera_info['cap']
                        if not cap or not camera_info['is_connected']:
                            print(f"ğŸ›‘ ì¹´ë©”ë¼ ì—°ê²° ì¢…ë£Œ: {thread_name}")
                            break
                        stream_count = camera_info['stream_count']
                except Exception as lock_error:
                    print(f"âš ï¸ ë½ ì˜¤ë¥˜: {lock_error}")
                    time.sleep(1.0)
                    continue
                
                # ì•„ë¬´ë„ ë³´ê³  ìˆì§€ ì•Šìœ¼ë©´ í”„ë ˆì„ ì½ê¸° ì¤‘ë‹¨
                is_background = self.background_streaming.get(rtsp_url, False)
                if stream_count <= 0 and not is_background:
                    # í ë¹„ìš°ê¸°
                    while not frame_queue.empty():
                        try:
                            frame_queue.get_nowait()
                        except:
                            break
                    time.sleep(0.5)
                    continue
                
                try:
                    current_time = time.time()
                    
                    # FFmpeg ì•ˆì „ì„±ì„ ìœ„í•œ ì¹´ë©”ë¼ ìƒíƒœ ì¬í™•ì¸
                    with camera_info['lock']:
                        if not camera_info['is_connected'] or not camera_info['cap']:
                            break
                        cap_ref = camera_info['cap']  # ë ˆí¼ëŸ°ìŠ¤ ë³µì‚¬ë¡œ ì•ˆì •ì„± í–¥ìƒ
                    
                    # í”„ë ˆì„ ì½ê¸° (ë¹„ë™ê¸°ì  grab)
                    try:
                        ret = cap_ref.grab()
                    except Exception as grab_error:
                        if current_time - last_error_log > 5.0:  # 5ì´ˆë§ˆë‹¤ ë¡œê·¸
                            print(f"âš ï¸ í”„ë ˆì„ grab ì˜¤ë¥˜: {grab_error}")
                            last_error_log = current_time
                        ret = False
                    
                    if ret:
                        # ìµœì‹  í”„ë ˆì„ë§Œ ìœ ì§€ (í í¬ê¸° ì²´í¬)
                        if frame_queue.qsize() >= 3:
                            # íê°€ 3ê°œ ì´ìƒì´ë©´ í•˜ë‚˜ ë¹¼ê³  ìƒˆë¡œ ë„£ê¸°
                            try:
                                old_frame = frame_queue.get_nowait()
                            except queue.Empty:
                                pass
                        
                        # í”„ë ˆì„ retrieve (ì•ˆì „í•˜ê²Œ)
                        try:
                            ret, frame = cap_ref.retrieve()
                        except Exception as retrieve_error:
                            if current_time - last_error_log > 5.0:
                                print(f"âš ï¸ í”„ë ˆì„ retrieve ì˜¤ë¥˜: {retrieve_error}")
                                last_error_log = current_time
                            ret, frame = False, None
                        
                        if ret and frame is not None and frame.size > 0:
                            consecutive_failures = 0
                            frame_skip_counter += 1
                            last_frame_time = current_time
                            
                            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                            timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
                            
                            # í™”ë©´ì— íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì‹œ (ë””ë²„ê¹…ìš©, í•„ìš”ì‹œ ì£¼ì„ ì²˜ë¦¬)
                            # cv2.putText(frame, timestamp_str, (10, 30), 
                            #            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                            
                            frame_data = {
                                'frame': frame,
                                'timestamp': current_time,
                                'timestamp_str': timestamp_str
                            }
                            
                            # ìƒˆ í”„ë ˆì„ ì¶”ê°€
                            try:
                                frame_queue.put_nowait(frame_data)
                            except queue.Full:
                                # íê°€ ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±° í›„ ì¶”ê°€
                                try:
                                    frame_queue.get_nowait()
                                    frame_queue.put_nowait(frame_data)
                                except:
                                    pass
                            
                            # FPS ê³„ì‚°
                            with camera_info['lock']:
                                camera_info['fps_counter'] += 1
                                if current_time - camera_info['last_fps_time'] >= 1.0:
                                    camera_info['avg_fps'] = camera_info['fps_counter']
                                    camera_info['fps_counter'] = 0
                                    camera_info['last_fps_time'] = current_time
                        else:
                            # í”„ë ˆì„ retrieve ì‹¤íŒ¨ ì‹œ ì—°ì† ì‹¤íŒ¨ ì¹´ìš´í„° ì¦ê°€
                            consecutive_failures += 1
                    else:
                        # grab ì‹¤íŒ¨ ì‹œ ì—°ì† ì‹¤íŒ¨ ì¹´ìš´í„° ì¦ê°€
                        consecutive_failures += 1
                    
                    # ì—°ì† ì‹¤íŒ¨ ì²´í¬
                    if consecutive_failures > 15:  # ì—°ì† ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜ ì´ˆê³¼
                        print(f"â¹ï¸ ì—°ì† ì‹¤íŒ¨ ì´ˆê³¼ - ìŠ¤ë ˆë“œ ì¢…ë£Œ: {thread_name}")
                        break
                    
                    # CPU ì‚¬ìš©ëŸ‰ ì œì–´ ë° FFmpeg ì•ˆì •ì„±
                    # time.sleep(0.02)  # 50 FPS ì œí•œ
                    
                except Exception as thread_error:
                    if current_time - last_error_log > 10.0:  # 10ì´ˆë§ˆë‹¤ ì˜¤ë¥˜ ë¡œê·¸
                        print(f"âŒ í”„ë ˆì„ ë¦¬ë” ì˜¤ë¥˜ ({thread_name}): {thread_error}")
                        last_error_log = current_time
                        import traceback
                        traceback.print_exc()
                    
                    consecutive_failures += 1
                    if consecutive_failures > 15:  # ì—°ì† ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜ ì´ˆê³¼
                        print(f"â¹ï¸ ì—°ì† ì‹¤íŒ¨ ì´ˆê³¼ - ìŠ¤ë ˆë“œ ì¢…ë£Œ: {thread_name}")
                        break
                    
                    # ì˜¤ë¥˜ í›„ ì§§ì€ ëŒ€ê¸°
                    time.sleep(0.5)
                    
        except Exception as critical_error:
            print(f"âŒ í”„ë ˆì„ ë¦¬ë” ìŠ¤ë ˆë“œ ì¹˜ëª…ì  ì˜¤ë¥˜: {critical_error}")
            import traceback
            traceback.print_exc()
            
        finally:
            print(f"ğŸ“º í”„ë ˆì„ ë¦¬ë” ì¢…ë£Œ: {thread_name} ({rtsp_url})")
            # ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹œ ì •ë¦¬
            try:
                # ì¹´ë©”ë¼ ì—°ê²° ìƒíƒœ ì—…ë°ì´íŠ¸
                if camera_info:
                    with camera_info['lock']:
                        camera_info['is_connected'] = False
                
                # ìŠ¤ë ˆë“œ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±°
                if rtsp_url in self.reader_threads:
                    del self.reader_threads[rtsp_url]
                    
                # ë‚¨ì€ í”„ë ˆì„ í ì •ë¦¬
                while not frame_queue.empty():
                    try:
                        frame_queue.get_nowait()
                    except:
                        break
                        
            except Exception as cleanup_error:
                print(f"âš ï¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {cleanup_error}")

    def flush_camera_buffer(self, rtsp_url):
        """ìˆ˜ë™ìœ¼ë¡œ ì¹´ë©”ë¼ ë²„í¼ ë¹„ìš°ê¸°. ìë™ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠìŒ"""
        camera_info = self.cameras.get(rtsp_url)
        if not camera_info:
            return False
        
        with camera_info['lock']:
            cap = camera_info['cap']
            if not cap:
                return False
            
            print(f"ğŸ”„ ë²„í¼ í”ŒëŸ¬ì‹œ ì‹œì‘: {rtsp_url}")
            frames_flushed = 0
            flush_start = time.time()
            
            # ìµœëŒ€ 1ì´ˆ ë™ì•ˆ ë²„í¼ ë¹„ìš°ê¸°
            while time.time() - flush_start < 1.0:
                ret = cap.grab()
                if not ret:
                    break
                frames_flushed += 1
                
                if frames_flushed >= 30:  # ìµœëŒ€ 30í”„ë ˆì„
                    break
            
            print(f"âœ… ë²„í¼ í”ŒëŸ¬ì‹œ ì™„ë£Œ: {frames_flushed}ê°œ í”„ë ˆì„ ì œê±°")
            return True
        
    def generate_frames(self, rtsp_url):
        """ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° - FFmpeg ì•ˆì •ì„± ê°•í™” ë²„ì „"""
        stream_id = f"stream_{id(threading.current_thread())}"
        print(f"ğŸ“¹ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {stream_id} ({rtsp_url})")
        
        try:
            camera_info = self.get_camera_stream(rtsp_url)
            frame_queue = self.frame_queues.get(rtsp_url)
            
            if not camera_info or not frame_queue:
                print(f"âŒ ì¹´ë©”ë¼ ì •ë³´ ë˜ëŠ” íê°€ ì—†ìŒ: {rtsp_url}")
                return
            
            # ìŠ¤íŠ¸ë¦¼ ì¹´ìš´í„° ì¦ê°€ (ì•ˆì „í•˜ê²Œ)
            try:
                with camera_info['lock']:
                    camera_info['stream_count'] += 1
                    current_streams = camera_info['stream_count']
                print(f"ğŸ“Š ìŠ¤íŠ¸ë¦¼ ì¹´ìš´í„° ì¦ê°€: {current_streams}ê°œ ({stream_id})")
            except Exception as lock_error:
                print(f"âš ï¸ ìŠ¤íŠ¸ë¦¼ ì¹´ìš´í„° ì¦ê°€ ì˜¤ë¥˜: {lock_error}")
                return
                
        except Exception as e:
            print(f"âŒ generate_frames ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return
        
        last_frame = None
        error_count = 0
        
        # ìŠ¤íŠ¸ë¦¬ë° ë©”ì¸ ë£¨í”„
        error_count = 0
        max_errors = 10
        last_error_time = 0
        
        try:
            while True:
                try:
                    # ì¹´ë©”ë¼ ì—°ê²° ìƒíƒœ í™•ì¸
                    connection_result = self.connect_camera(rtsp_url)
                    
                    if not connection_result:
                        error_count += 1
                        if error_count > max_errors:
                            print(f"âŒ ì—°ê²° ì‹¤íŒ¨ ì´ˆê³¼ - ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ: {stream_id}")
                            break
                            
                        yield (b'--frame\r\n'
                               b'Content-Type: image/jpeg\r\n\r\n' + 
                               self.get_error_frame("Camera Disconnected") + b'\r\n')
                        time.sleep(2)
                        continue
                    
                    # ì—°ê²° ì„±ê³µ ì‹œ ì˜¤ë¥˜ ì¹´ìš´í„° ë¦¬ì…‹
                    error_count = 0
                    
                except Exception as connect_error:
                    current_time = time.time()
                    if current_time - last_error_time > 5.0:
                        print(f"âš ï¸ ì—°ê²° ì‹œë„ ì˜¤ë¥˜: {connect_error}")
                        last_error_time = current_time
                    time.sleep(1)
                    continue
                
                try:
                    # íì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸° íƒ€ì„ì•„ì›ƒ)
                    try:
                        frame_data = frame_queue.get(timeout=1.0)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                    except queue.Empty:
                        # íê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                        continue
                    
                    # dict í˜•ì‹ì¸ì§€ í™•ì¸í•˜ê³  í”„ë ˆì„ ì¶”ì¶œ
                    if isinstance(frame_data, dict):
                        frame = frame_data.get('frame')
                        timestamp_str = frame_data.get('timestamp_str', '')
                        
                        # ë””ë²„ê¹…ìš© ë¡œê·¸ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
                        # print(f"ğŸ“º ìŠ¤íŠ¸ë¦¬ë° í”„ë ˆì„: {timestamp_str}")
                    else:
                        # êµ¬ë²„ì „ í˜¸í™˜ì„± (í”„ë ˆì„ë§Œ ìˆëŠ” ê²½ìš°)
                        frame = frame_data
                    
                    if frame is None:
                        continue
                    
                    last_frame = frame
                    error_count = 0
                    
                except Exception as frame_error:
                    current_time = time.time()
                    if current_time - last_error_time > 3.0:
                        print(f"âš ï¸ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {frame_error}")
                        last_error_time = current_time
                    
                    # ë§ˆì§€ë§‰ í”„ë ˆì„ ì‚¬ìš© ë˜ëŠ” ì˜¤ë¥˜ í”„ë ˆì„ ì „ì†¡
                    if last_frame is not None:
                        frame = last_frame
                    else:
                        yield (b'--frame\r\n'
                               b'Content-Type: image/jpeg\r\n\r\n' + 
                               self.get_error_frame("No Signal") + b'\r\n')
                        continue
                
                # JPEG ì¸ì½”ë”©
                encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 70]
                _, buffer = cv2.imencode('.jpg', frame, encode_param)
                frame_bytes = buffer.tobytes()
                
                yield (b'--frame\r\n'
                       b'Content-Type: image/jpeg\r\n'
                       b'Content-Length: ' + f'{len(frame_bytes)}'.encode() + b'\r\n\r\n' + 
                       frame_bytes + b'\r\n')
                
                # FPS ì œì–´ (ë„ˆë¬´ ë¹ ë¥¸ ì „ì†¡ ë°©ì§€)
                # time.sleep(0.033)  # 30 FPS ì œí•œ
                
        except GeneratorExit:
            pass
        except Exception as stream_error:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {stream_error}")
            import traceback
            traceback.print_exc()
            
        finally:
            print(f"ğŸ“¹ ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ: {stream_id} ({rtsp_url})")
            
            # ìŠ¤íŠ¸ë¦¼ ì¹´ìš´í„° ê°ì†Œ (ì•ˆì „í•˜ê²Œ)
            try:
                with camera_info['lock']:
                    camera_info['stream_count'] -= 1
                    remaining_streams = camera_info['stream_count']
                    is_background = self.background_streaming.get(rtsp_url, False)
                    
                    print(f"ğŸ“Š ìŠ¤íŠ¸ë¦¼ ì¹´ìš´í„° ê°ì†Œ: {remaining_streams}ê°œ ë‚¨ìŒ")
                    
                    # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ì´ ì¢…ë£Œë˜ê³  ë°±ê·¸ë¼ìš´ë“œê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                    if remaining_streams <= 0 and not is_background:
                        print(f"ğŸ§¹ ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ìë™ ì •ë¦¬: {rtsp_url}")
                        
                        # ì¹´ë©”ë¼ ì—°ê²° í•´ì œ
                        if camera_info['cap']:
                            try:
                                camera_info['cap'].release()
                            except:
                                pass
                            camera_info['cap'] = None
                        camera_info['is_connected'] = False
                        
                        # í”„ë ˆì„ í ë¹„ìš°ê¸°
                        try:
                            while not frame_queue.empty():
                                frame_queue.get_nowait()
                        except:
                            pass
                            
            except Exception as cleanup_error:
                print(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì •ë¦¬ ì˜¤ë¥˜: {cleanup_error}")
    
    def get_error_frame(self, message="Camera Error"):
        """ì—ëŸ¬ ë©”ì‹œì§€ê°€ í¬í•¨ëœ í”„ë ˆì„ ìƒì„±"""
        error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
        
        # ë°°ê²½ìƒ‰ ì„¤ì • (ì–´ë‘ìš´ íšŒìƒ‰)
        error_frame.fill(30)
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1
        thickness = 2
        text_size = cv2.getTextSize(message, font, font_scale, thickness)[0]
        
        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚° (ì¤‘ì•™)
        text_x = (error_frame.shape[1] - text_size[0]) // 2
        text_y = (error_frame.shape[0] + text_size[1]) // 2
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        cv2.putText(error_frame, message, (text_x, text_y), 
                   font, font_scale, (0, 0, 255), thickness)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        cv2.putText(error_frame, timestamp, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        _, buffer = cv2.imencode('.jpg', error_frame)
        return buffer.tobytes()
    
    def get_camera_status(self, rtsp_url):
        camera_info = self.get_camera_stream(rtsp_url)
        with camera_info['lock']:
            return {
                'is_connected': camera_info['is_connected'],
                'avg_fps': camera_info['avg_fps'],
                'tracker_count': camera_info['tracker_count'],
                'stream_count': camera_info['stream_count'],
                'reconnect_attempts': camera_info['reconnect_attempts']
            }
    
    def cleanup_camera(self, rtsp_url):
        """ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ì•ˆì „í•œ ë²„ì „)"""
        print(f"ğŸ§¹ ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘: {rtsp_url}")
        
        try:
            # ë½ íšë“ ì‹œë„ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            if not self.global_lock.acquire(timeout=3.0):
                print(f"âš ï¸ ì¹´ë©”ë¼ ì •ë¦¬ ë½ íƒ€ì„ì•„ì›ƒ: {rtsp_url}")
                return False
            
            try:
                if rtsp_url in self.cameras:
                    camera_info = self.cameras[rtsp_url]
                    
                    # ì¹´ë©”ë¼ ì—°ê²° í•´ì œ (ì•ˆì „í•˜ê²Œ)
                    try:
                        with camera_info['lock']:
                            if camera_info['cap']:
                                camera_info['cap'].release()
                                camera_info['cap'] = None
                            camera_info['is_connected'] = False
                    except Exception as e:
                        print(f"âš ï¸ ì¹´ë©”ë¼ ì—°ê²° í•´ì œ ì˜¤ë¥˜: {e}")
                    
                    # ìŠ¤ë ˆë“œ ì¢…ë£Œ (ë¹„ë™ê¸°ì ìœ¼ë¡œ)
                    if rtsp_url in self.reader_threads:
                        thread = self.reader_threads[rtsp_url]
                        if thread.is_alive():
                            print(f"ğŸ”„ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°: {rtsp_url}")
                            try:
                                thread.join(timeout=1.0)  # ì§§ì€ íƒ€ì„ì•„ì›ƒ
                                if thread.is_alive():
                                    print(f"âš ï¸ ìŠ¤ë ˆë“œ ê°•ì œ ì¢…ë£Œ: {rtsp_url}")
                            except Exception as e:
                                print(f"âš ï¸ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì˜¤ë¥˜: {e}")
                        del self.reader_threads[rtsp_url]
                    
                    # í ì •ë¦¬
                    if rtsp_url in self.frame_queues:
                        try:
                            # íì— ë‚¨ì€ ë°ì´í„° ë¹„ìš°ê¸°
                            while not self.frame_queues[rtsp_url].empty():
                                try:
                                    self.frame_queues[rtsp_url].get_nowait()
                                except:
                                    break
                            del self.frame_queues[rtsp_url]
                        except Exception as e:
                            print(f"âš ï¸ í”Œë ˆì„ í ì •ë¦¬ ì˜¤ë¥˜: {e}")
                    
                    # ì¹´ë©”ë¼ ì •ë³´ ì‚­ì œ
                    del self.cameras[rtsp_url]
                    print(f"âœ… ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ: {rtsp_url}")
                    return True
                else:
                    print(f"âš ï¸ ì¹´ë©”ë¼ ì •ë³´ ì—†ìŒ: {rtsp_url}")
                    return False
                    
            finally:
                self.global_lock.release()
                
        except Exception as e:
            print(f"âŒ ì¹´ë©”ë¼ ì •ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def start_background_streaming(self, rtsp_url):
        """ë°±ê·¸ë¼ìš´ë“œ ì—°ì† ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘"""
        print(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì‹œë„: {rtsp_url}")
        
        # ë½ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë°ë“œë½ ë°©ì§€
        if self.global_lock.acquire(timeout=3.0):
            try:
                self.background_streaming[rtsp_url] = True
                print(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° í”Œë˜ê·¸ ì„¤ì •: {rtsp_url}")
            finally:
                self.global_lock.release()
        else:
            print(f"âš ï¸ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ë½ íƒ€ì„ì•„ì›ƒ: {rtsp_url}")
            return False
            
        # ì¹´ë©”ë¼ ì—°ê²° í™•ì¸ ë° ìŠ¤íŠ¸ë¦¼ ì‹œì‘ (ë½ ì™¸ë¶€ì—ì„œ)
        if self.connect_camera(rtsp_url):
            print(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”: {rtsp_url}")
            return True
        else:
            print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ (ì—°ê²° ë¶ˆê°€): {rtsp_url}")
            return False
    
    def stop_background_streaming(self, rtsp_url):
        """ë°±ê·¸ë¼ìš´ë“œ ì—°ì† ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€"""
        with self.global_lock:
            if rtsp_url in self.background_streaming:
                self.background_streaming[rtsp_url] = False
                del self.background_streaming[rtsp_url]
                print(f"â¹ï¸ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€: {rtsp_url}")
    
    def is_background_streaming(self, rtsp_url):
        """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ í™•ì¸"""
        return self.background_streaming.get(rtsp_url, False)
    
    def start_all_background_streaming(self):
        """ëª¨ë“  ì¹´ë©”ë¼ì˜ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (DBì—ì„œ ì‹¤ì‹œê°„ ì¡°íšŒ)"""
        from .models import Camera
        cameras = Camera.objects.all()  # ë§¤ë²ˆ ìµœì‹  ì¹´ë©”ë¼ ëª©ë¡ì„ ê°€ì ¸ì˜´
        
        print(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: ì´ {cameras.count()}ê°œ ì¹´ë©”ë¼")
        
        for camera in cameras:
            try:
                self.start_background_streaming(camera.rtsp_url)
                print(f"âœ… ì¹´ë©”ë¼ '{camera.name}' ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
            except Exception as e:
                print(f"âŒ ì¹´ë©”ë¼ '{camera.name}' ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
    
    def stop_all_background_streaming(self):
        """ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€"""
        rtsp_urls = list(self.background_streaming.keys())
        for rtsp_url in rtsp_urls:
            self.stop_background_streaming(rtsp_url)
    
    def cleanup_all_resources(self):
        """ëª¨ë“  ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)"""
        print("ğŸ§¹ ëª¨ë“  ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
        
        # ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€
        self.stop_all_background_streaming()
        
        # ëª¨ë“  ì¹´ë©”ë¼ ì—°ê²° í•´ì œ
        with self.global_lock:
            rtsp_urls = list(self.cameras.keys())
            for rtsp_url in rtsp_urls:
                self.cleanup_camera(rtsp_url)
        
        print("âœ… ëª¨ë“  ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

class AIDetectionSystem:
    def __init__(self):
        self.yolo_model = None
        self.clip_model = None
        self.clip_preprocess = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.detection_threads = {}
        self.detection_active = {}
        self.screenshot_dir = os.path.join(settings.MEDIA_ROOT, 'screenshots')
        self.load_models()
        self.ensure_screenshot_dir()
        
        # ì „ì—­ í ì‚¬ìš©
        self.alert_queue = GLOBAL_ALERT_QUEUE
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        self.setup_korean_font()
        # ì „ì²´ ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        self.all_detection_dir = os.path.join(settings.MEDIA_ROOT, 'all_detections')
        self.ensure_all_detection_dir()

    def setup_korean_font(self):
        """í•œê¸€ í°íŠ¸ ì„¤ì •"""
        # Windows í™˜ê²½
        font_paths = [
            "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”•
            "C:/Windows/Fonts/gulim.ttc",   # êµ´ë¦¼
            "C:/Windows/Fonts/batang.ttc",  # ë°”íƒ•
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",  # Linux
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",  # macOS
        ]
        
        self.korean_font = None
        for font_path in font_paths:
            if os.path.exists(font_path):
                try:
                    self.korean_font = font_path
                    print(f"âœ… í•œê¸€ í°íŠ¸ ë¡œë“œ: {font_path}")
                    break
                except:
                    continue
        
        if not self.korean_font:
            print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë¬¸ë§Œ í‘œì‹œë©ë‹ˆë‹¤.")
    
    def ensure_screenshot_dir(self):
        """ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.screenshot_dir):
            os.makedirs(self.screenshot_dir, exist_ok=True)
    
    def ensure_all_detection_dir(self):
        """ëª¨ë“  íƒì§€ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.all_detection_dir):
            os.makedirs(self.all_detection_dir, exist_ok=True)
    
    def load_models(self):
        """YOLO11 ë° CLIP ëª¨ë¸ ë¡œë“œ (ë””ë²„ê·¸ ì¶”ê°€)"""
        print("\nğŸ”§ AI ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
        print(f"  - PyTorch ë²„ì „: {torch.__version__}")
        print(f"  - CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"  - CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}")
        
        try:
            # ë¦¬ëˆ…ìŠ¤/ìš°ë¶„íˆ¬ í™˜ê²½ì—ì„œ ë””ìŠ¤í”Œë ˆì´ ì„œë²„ ì—†ì´ OpenCV ì‹¤í–‰ ì„¤ì •
            import platform
            if platform.system() == 'Linux':
                os.environ['DISPLAY'] = ':0'  # X11 ë””ìŠ¤í”Œë ˆì´ ì„¤ì •
                # í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì—ì„œ GUI ë°±ì—”ë“œ ë¹„í™œì„±í™”
                import matplotlib
                matplotlib.use('Agg')
            
            # YOLO11 ëª¨ë¸ ë¡œë“œ
            yolo_path = os.path.join(settings.BASE_DIR, 'CCTV', 'yolo11l.pt')
            print(f"  - YOLO ëª¨ë¸ ê²½ë¡œ: {yolo_path}")
            print(f"  - YOLO ëª¨ë¸ ì¡´ì¬: {os.path.exists(yolo_path)}")
            
            if os.path.exists(yolo_path):
                # í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì—ì„œ YOLO ëª¨ë¸ ë¡œë“œ ì‹œ verbose=False ì„¤ì •
                # self.yolo_model = YOLO(yolo_path)
                self.yolo_model = YOLO()
                # GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° CPUë¡œ ê°•ì œ ì„¤ì •
                if not torch.cuda.is_available():
                    self.device = "cpu"
                print(f"âœ… YOLO11 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {yolo_path} (device: {self.device})")
                
                # YOLO í´ë˜ìŠ¤ ì •ë³´ ì¶œë ¥
                if hasattr(self.yolo_model, 'model') and hasattr(self.yolo_model.model, 'names'):
                    print(f"  - YOLO í´ë˜ìŠ¤ ìˆ˜: {len(self.yolo_model.model.names)}")
                    # print(f"  - YOLO ì£¼ìš” í´ë˜ìŠ¤: {list(self.yolo_model.model.names.values())[:10]}...")
                    print(f"  - YOLO ì£¼ìš” í´ë˜ìŠ¤: {list(self.yolo_model.model.names.values())}...")
            else:
                print(f"âŒ YOLO11 ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {yolo_path}")
            
            # CLIP ëª¨ë¸ ë¡œë“œ
            try:
                print(f"\n  - CLIP ëª¨ë¸ ë¡œë“œ ì¤‘...")
                self.clip_model, self.clip_preprocess = clip.load("ViT-L/14@336px", device=self.device)
                print(f"âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {self.device})")
            except Exception as clip_error:
                # CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ CPUë¡œ ì¬ì‹œë„
                print(f"âš ï¸ CLIP GPU ë¡œë“œ ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„: {clip_error}")
                self.device = "cpu"
                self.clip_model, self.clip_preprocess = clip.load("ViT-L/14@336px", device=self.device)
                print(f"âœ… CLIP ëª¨ë¸ CPU ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ ì‹œìŠ¤í…œì´ ê³„ì† ë™ì‘í•˜ë„ë¡ ì„¤ì •
            self.yolo_model = None
            self.clip_model = None
    
    def start_detection_for_camera(self, camera):
        """íŠ¹ì • ì¹´ë©”ë¼ì— ëŒ€í•œ íƒì§€ ì‹œì‘ - ê¸°ì¡´ ìŠ¤ë ˆë“œ ì™„ì „ ì¢…ë£Œ í™•ì¸ í›„ ì‹œì‘"""
        # ê¸°ì¡´ ìŠ¤ë ˆë“œê°€ ìˆë‹¤ë©´ ì™„ì „íˆ ì¢…ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        if camera.id in self.detection_threads:
            old_thread = self.detection_threads[camera.id]
            if old_thread and old_thread.is_alive():
                print(f"â³ ì¹´ë©”ë¼ '{camera.name}' ê¸°ì¡´ íƒì§€ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°...")
                # í”Œë˜ê·¸ë¥¼ Falseë¡œ ì„¤ì •
                self.detection_active[camera.id] = False
                # ìŠ¤ë ˆë“œê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
                old_thread.join(timeout=5.0)
                if old_thread.is_alive():
                    print(f"âš ï¸ ì¹´ë©”ë¼ '{camera.name}' ê¸°ì¡´ ìŠ¤ë ˆë“œ ê°•ì œ ì¢…ë£Œ (íƒ€ì„ì•„ì›ƒ)")
                else:
                    print(f"âœ… ì¹´ë©”ë¼ '{camera.name}' ê¸°ì¡´ ìŠ¤ë ˆë“œ ì •ìƒ ì¢…ë£Œë¨")
        
        # ìƒˆë¡œìš´ íƒì§€ ìŠ¤ë ˆë“œ ì‹œì‘
        self.detection_active[camera.id] = True
        detection_thread = threading.Thread(
            target=self._detection_worker,
            args=(camera,),
            daemon=True,
            name=f"Detection-{camera.name}-{camera.id}"
        )
        self.detection_threads[camera.id] = detection_thread
        detection_thread.start()
        print(f"ğŸ¯ ì¹´ë©”ë¼ '{camera.name}' ìƒˆë¡œìš´ íƒì§€ ìŠ¤ë ˆë“œ ì‹œì‘")
    
    def stop_detection_for_camera(self, camera_id):
        """íŠ¹ì • ì¹´ë©”ë¼ì— ëŒ€í•œ íƒì§€ ì¤‘ì§€"""
        if camera_id in self.detection_active:
            self.detection_active[camera_id] = False
            print(f"â¹ï¸ ì¹´ë©”ë¼ ID {camera_id} íƒì§€ ì¤‘ì§€")
    
    def _detection_worker(self, camera):
        """ì¹´ë©”ë¼ë³„ íƒì§€ ì›Œì»¤ - íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì‹œ ë²„ì „"""
        from .models import TargetLabel, DetectionLog
        
        print(f"\nğŸš€ íƒì§€ ì›Œì»¤ ì‹œì‘: ì¹´ë©”ë¼ '{camera.name}' (ID: {camera.id})")
        last_detection_time = time.time()
        
        while self.detection_active.get(camera.id, False):
            try:
                camera_info = camera_streamer.get_camera_stream(camera.rtsp_url)
                
                if not camera_info['is_connected']:
                    print(f"âš ï¸ ì¹´ë©”ë¼ '{camera.name}' ì—°ê²°ë˜ì§€ ì•ŠìŒ")
                    time.sleep(2)
                    continue
                
                frame_queue = camera_streamer.frame_queues.get(camera.rtsp_url)
                if not frame_queue:
                    time.sleep(1)
                    continue
                
                # í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                frame_data = None
                try:
                    frame_data = frame_queue.get(timeout=1.0)
                except queue.Empty:
                    time.sleep(0.5)
                    continue
                
                # í”„ë ˆì„ ë°ì´í„° ì¶”ì¶œ
                if isinstance(frame_data, dict):
                    frame = frame_data.get('frame')
                    frame_timestamp = frame_data.get('timestamp_str', 'Unknown')
                    frame_age = time.time() - frame_data.get('timestamp', time.time())
                else:
                    # êµ¬ë²„ì „ í˜¸í™˜ì„±
                    frame = frame_data
                    frame_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    frame_age = 0
                
                if frame is None:
                    continue
                
                # í”„ë ˆì„ ì •ë³´ ì¶œë ¥
                # current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
                # print(f"\nğŸ“¹ í”„ë ˆì„ íšë“: ì¹´ë©”ë¼ '{camera.name}'")
                # print(f"   - í”„ë ˆì„ ìº¡ì²˜ ì‹œê°„: {frame_timestamp}")
                # print(f"   - í˜„ì¬ ì²˜ë¦¬ ì‹œê°„: {current_time}")
                # print(f"   - í”„ë ˆì„ ì§€ì—°: {frame_age:.2f}ì´ˆ")
                # print(f"   - í”„ë ˆì„ í¬ê¸°: {frame.shape}")
                
                # í”„ë ˆì„ì´ ë„ˆë¬´ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ
                if frame_age > 5.0:
                    print(f"   âš ï¸ í”„ë ˆì„ì´ ë„ˆë¬´ ì˜¤ë˜ë¨ ({frame_age:.1f}ì´ˆ), ìŠ¤í‚µ")
                    continue
                
                # ë§¤ ë£¨í”„ë§ˆë‹¤ ì¹´ë©”ë¼ì™€ íƒ€ê²Ÿ ë¼ë²¨ ì •ë³´ë¥¼ DBì—ì„œ ìƒˆë¡œ ê°€ì ¸ì˜¤ê¸° (ì¤‘ìš”!)
                try:
                    from .models import Camera
                    camera = Camera.objects.prefetch_related('target_labels').get(id=camera.id)
                    target_labels = list(camera.target_labels.all())
                except Camera.DoesNotExist:
                    print(f"âŒ ì¹´ë©”ë¼ ID {camera.id}ê°€ ì‚­ì œë¨ - íƒì§€ ì¤‘ì§€")
                    break
                
                if not target_labels:
                    print(f"âš ï¸ ì¹´ë©”ë¼ '{camera.name}'ì— íƒ€ê²Ÿ ë¼ë²¨ì´ ì—†ìŒ")
                    time.sleep(5)
                    continue
                
                print(f"ğŸ¯ íƒ€ê²Ÿ ë¼ë²¨ {len(target_labels)}ê°œë¡œ íƒì§€ ì‹œì‘")
                
                # íƒì§€ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                detection_start = time.time()
                
                # ê°ì²´ íƒì§€ ìˆ˜í–‰
                # detections = self._detect_objects(frame, target_labels)
                detections = self._detect_objects(frame, target_labels, camera)

                # íƒì§€ ì†Œìš” ì‹œê°„
                detection_duration = time.time() - detection_start
                print(f"â±ï¸ íƒì§€ ì†Œìš” ì‹œê°„: {detection_duration:.2f}ì´ˆ")
                
                # íƒì§€ ê²°ê³¼ ì²˜ë¦¬
                if detections:
                    # print(f"âœ¨ íƒì§€ ì™„ë£Œ! {len(detections)}ê°œ íƒ€ê²Ÿ ë°œê²¬ (ì‹œê°„: {current_time})")
                    for detection in detections:
                        self._process_detection(camera, frame, detection, target_labels)
                        # ë§¤ íƒì§€ë§ˆë‹¤ ë³„ë„ ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                        self._save_all_detection_screenshot(camera, frame, detection)
                else:
                    pass
                
                # íƒì§€ ê°„ê²© ê³„ì‚° ë° í‘œì‹œ
                # time_since_last = time.time() - last_detection_time
                # print(f"ğŸ“Š íƒì§€ ì£¼ê¸°: {time_since_last:.1f}ì´ˆ")
                # last_detection_time = time.time()
                
                # íƒì§€ ê°„ê²©
                time.sleep(1.5)
                
            except Exception as e:
                print(f"âŒ íƒì§€ ì›Œì»¤ ì˜¤ë¥˜ (ì¹´ë©”ë¼: {camera.name}): {e}")
                import traceback
                traceback.print_exc()
                time.sleep(2)
        
        print(f"ğŸ›‘ íƒì§€ ì›Œì»¤ ì¢…ë£Œ: ì¹´ë©”ë¼ '{camera.name}'")

    def _detect_objects(self, frame, target_labels, camera):
        """
        person ê°ì²´ë§Œ íƒì§€í•˜ëŠ” Softmax ë°©ì‹ ê°ì²´ íƒì§€
        - YOLOì—ì„œ person í´ë˜ìŠ¤ë§Œ í•„í„°ë§
        - CLIPì´ ëª¨ë“  ë¼ë²¨ + "other object"ë¥¼ ë™ì‹œì— ë¹„êµ
        - person ê°ì²´ íƒì§€ ì‹œ 15% í™•ì¥ëœ ì˜ì—­ ì‚¬ìš©
        """
        detections = []
        
        # ì„ê³„ì¹˜ ì„¤ì •
        YOLO_CANDIDATE_THRESHOLD = 0.5   # YOLO í›„ë³´ ë°•ìŠ¤ ì„ê³„ì¹˜
        CLIP_CONFIDENCE_THRESHOLD = 0.73   # CLIP softmax ìµœì†Œ ì‹ ë¢°ë„
        
        if self.yolo_model is None or self.clip_model is None:
            print("âš ï¸ YOLO ë˜ëŠ” CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return detections
        
        try:
            # 1. YOLOë¡œ í›„ë³´ ë°•ìŠ¤ ì¶”ì¶œ
            results = self.yolo_model(frame, conf=YOLO_CANDIDATE_THRESHOLD, imgsz=960)
            
            if not results or len(results) == 0:
                return detections
            
            yolo_result = results[0]
            
            if not hasattr(yolo_result, 'boxes') or yolo_result.boxes is not None:
                boxes = yolo_result.boxes.xyxy.cpu().numpy()
                confidences = yolo_result.boxes.conf.cpu().numpy() if yolo_result.boxes.conf is not None else []
                classes = yolo_result.boxes.cls.cpu().numpy() if yolo_result.boxes.cls is not None else []
                
                if len(boxes) == 0:
                    return detections
                
                # YOLO í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                class_names = yolo_result.names if hasattr(yolo_result, 'names') else {}
                
                # person í´ë˜ìŠ¤ë§Œ í•„í„°ë§ (COCO ë°ì´í„°ì…‹ì—ì„œ personì€ í´ë˜ìŠ¤ 0)
                person_mask = classes == 0  # person í´ë˜ìŠ¤ ID
                
                if not person_mask.any():
                    print("ğŸ“Š YOLOì—ì„œ person ê°ì²´ë¥¼ ì°¾ì§€ ëª»í–ˆìŒ")
                    return detections
                    
                # person ê°ì²´ë§Œ í•„í„°ë§
                person_boxes = boxes[person_mask]
                person_confidences = confidences[person_mask]
                person_classes = classes[person_mask]
                
                print(f"ğŸ“Š YOLO person ê°ì²´: {len(person_boxes)}ê°œ íƒì§€")
                
                # 2. CLIPì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¤€ë¹„ (DB ë¼ë²¨ + "other object")
                text_queries = []
                label_indices = []  # ê° ì¿¼ë¦¬ê°€ ì–´ë–¤ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ”ì§€ ì¶”ì 
                
                # DBì—ì„œ ê°€ì ¸ì˜¨ ë¼ë²¨ë“¤
                for i, target_label in enumerate(target_labels):
                    query = f"a photo of {target_label.label_name}"
                    text_queries.append(query)
                    label_indices.append(i)
                
                # "other object" ì¶”ê°€ (í•­ìƒ ë§ˆì§€ë§‰)
                text_queries.append("other object")
                other_object_idx = len(text_queries) - 1
                
                # print(f"ë“±ë¡ëœ ê°ì²´ ì´ë¦„ : {[tl.display_name for tl in target_labels]}")
                # print(f"ğŸ¯ ë¹„êµí•  ë¼ë²¨: {[tl.label_name for tl in target_labels]} + 'other object'")
                
                # í…ìŠ¤íŠ¸ í† í°í™”
                text_tokens = clip.tokenize(text_queries).to(self.device)
                
                with torch.no_grad():
                    text_features = self.clip_model.encode_text(text_tokens)
                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # 3. ê° íƒ€ê²Ÿ ë¼ë²¨ë³„ë¡œ íƒì§€ëœ ë°•ìŠ¤ë“¤ì„ ìˆ˜ì§‘
                label_detections = {i: [] for i in range(len(target_labels))}
                
                # print(f"ğŸ”§ í˜„ì¬ CLIP_CONFIDENCE_THRESHOLD: {CLIP_CONFIDENCE_THRESHOLD}")

                # 4. ê° person ë°•ìŠ¤ì— ëŒ€í•´ CLIPìœ¼ë¡œ ë¶„ë¥˜
                for box_idx, (box, yolo_conf, cls) in enumerate(zip(person_boxes, person_confidences, person_classes)):
                    x1, y1, x2, y2 = map(int, box)
                    
                    # person ë°•ìŠ¤ í¬ê¸°ë¥¼ 20% í™•ì¥
                    box_scale_extend = 0.2

                    box_width = x2 - x1
                    box_height = y2 - y1
                    expand_w = int(box_width * box_scale_extend)
                    expand_h = int(box_height * box_scale_extend)
                    
                    # í”„ë ˆì„ ê²½ê³„ ë‚´ì—ì„œ í™•ì¥
                    frame_h, frame_w = frame.shape[:2]
                    x1_expanded = max(0, x1 - expand_w)
                    y1_expanded = max(0, y1 - expand_h)
                    x2_expanded = min(frame_w, x2 + expand_w)
                    y2_expanded = min(frame_h, y2 + expand_h)
                    
                    cropped_region = frame[y1_expanded:y2_expanded, x1_expanded:x2_expanded]
                    
                    if cropped_region.size == 0:
                        continue
                    
                    # person í´ë˜ìŠ¤ í™•ì¸
                    yolo_class = class_names.get(int(cls), 'person')
                    print(f"ğŸ‘¤ person ê°ì²´ ë°•ìŠ¤ {box_idx}: {yolo_class} (conf: {yolo_conf:.2f}, í™•ì¥: 15%)")
                    
                    # CLIPìœ¼ë¡œ ì´ë¯¸ì§€ ì¸ì½”ë”©
                    pil_crop = Image.fromarray(cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB))
                    crop_tensor = self.clip_preprocess(pil_crop).unsqueeze(0).to(self.device)
                    
                    with torch.no_grad():
                        crop_features = self.clip_model.encode_image(crop_tensor)
                        crop_features = crop_features / crop_features.norm(dim=-1, keepdim=True)
                        
                        # ëª¨ë“  í…ìŠ¤íŠ¸ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                        logits = (crop_features @ text_features.T) * 100.0  # CLIPì˜ temperature scaling
                        
                        # Softmax ì ìš©
                        probs = logits.softmax(dim=-1).cpu().numpy()[0]
                    
                    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë¼ë²¨ ì°¾ê¸°
                    best_idx = int(np.argmax(probs))
                    best_prob = float(probs[best_idx])
                    
                    # print(f"   Box{box_idx} [{yolo_class}]: ", end="")
                    # for i, (query, prob) in enumerate(zip(text_queries, probs)):
                    #     if i < len(target_labels):
                    #         print(f"{target_labels[i].display_name}={prob:.2f} ", end="")
                    #     else:
                    #         print(f"other={prob:.2f} ", end="")
                    
                    # "other object"ê°€ ìµœê³ ì ì´ë©´ ë¬´ì‹œ
                    if best_idx == other_object_idx:
                        # print(f"      âŒ 'other object'ë¡œ ë¶„ë¥˜ë¨ ({best_prob:.2f}) - ë¬´ì‹œ")
                        continue
                    
                    # ì‹ ë¢°ë„ê°€ ì„ê³„ì¹˜ ë¯¸ë§Œì´ë©´ ë¬´ì‹œ
                    if best_prob < CLIP_CONFIDENCE_THRESHOLD:
                        # print(f"      âŒ ì‹ ë¢°ë„ ë¶€ì¡± ({best_prob:.2f} < {CLIP_CONFIDENCE_THRESHOLD})")
                        continue
                    
                    # í•´ë‹¹ ë¼ë²¨ë¡œ ë¶„ë¥˜
                    label_idx = label_indices[best_idx]
                    target_label = target_labels[label_idx]
                    
                    print(f"      âœ… '{target_label.display_name}'ë¡œ íƒì§€! (ì‹ ë¢°ë„: {best_prob:.2f})")
                    
                    label_detections[label_idx].append({
                        'box': [x1, y1, x2, y2],
                        'confidence': best_prob,
                        'yolo_confidence': float(yolo_conf),
                        'clip_probability': best_prob,
                        'yolo_class': yolo_class
                    })
                
                # 5. ê° ë¼ë²¨ë³„ë¡œ íƒì§€ ê²°ê³¼ ìƒì„±
                for label_idx, detected_boxes in label_detections.items():
                    if detected_boxes:
                        target_label = target_labels[label_idx]
                        avg_confidence = sum(box['confidence'] for box in detected_boxes) / len(detected_boxes)
                        
                        detection = {
                            'label': target_label,
                            'confidence': float(avg_confidence),
                            'count': len(detected_boxes),
                            'has_alert': target_label.has_alert,
                            'boxes': detected_boxes
                        }
                        
                        detections.append(detection)
                        
                        # print(f"\nğŸ¯ {target_label.display_name} ìµœì¢… íƒì§€:")
                        # print(f"   - ë°•ìŠ¤ ìˆ˜: {len(detected_boxes)}ê°œ")
                        # print(f"   - í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.1%}")
                        # print(f"   - ê²½ê³  ì„¤ì •: {'í™œì„±' if target_label.has_alert else 'ë¹„í™œì„±'}")
                
                if not detections:
                    print(f"ğŸ’¤ íƒì§€ëœ person ê°ì²´ ì—†ìŒ (ëª¨ë‘ 'other object'ì´ê±°ë‚˜ ì‹ ë¢°ë„ ë¯¸ë‹¬)")
            
        except Exception as e:
            print(f"âŒ ê°ì²´ íƒì§€ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        return detections


    # def _detect_objects(self, frame, target_labels):
    #     """í”„ë ˆì„ì—ì„œ ê°ì²´ íƒì§€ - ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ í¬í•¨"""
    #     detections = []
        
    #     if self.yolo_model is None or self.clip_model is None:
    #         print("âš ï¸ ë””ë²„ê·¸: YOLO ë˜ëŠ” CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    #         return detections
        
    #     try:
    #         # YOLOë¡œ 1ì°¨ ê°ì²´ íƒì§€ (ë°”ìš´ë”© ë°•ìŠ¤ íšë“)
    #         results = self.yolo_model(frame, verbose=False)
            
    #         if not results or len(results) == 0:
    #             return detections
            
    #         yolo_result = results[0]
            
    #         if hasattr(yolo_result, 'boxes') and yolo_result.boxes is not None:
    #             boxes = yolo_result.boxes.xyxy.cpu().numpy()
    #             confidences = yolo_result.boxes.conf.cpu().numpy() if yolo_result.boxes.conf is not None else []
                
    #             # ì‹ ë¢°ë„ 0.6 ì´ìƒì¸ ë°”ìš´ë”© ë°•ìŠ¤ë§Œ ì‚¬ìš©
    #             high_conf_mask = confidences >= 0.2
    #             valid_boxes = boxes[high_conf_mask]
    #             valid_confidences = confidences[high_conf_mask]
                
    #             if len(valid_boxes) == 0:
    #                 return detections
                
    #             # ê° íƒ€ê²Ÿ ë¼ë²¨ì— ëŒ€í•´ CLIPìœ¼ë¡œ ë¶„ë¥˜
    #             for target_label in target_labels:
    #                 detected_boxes = []
                    
    #                 for idx, (box, yolo_conf) in enumerate(zip(valid_boxes, valid_confidences)):
    #                     x1, y1, x2, y2 = map(int, box)
    #                     cropped_region = frame[y1:y2, x1:x2]
                        
    #                     if cropped_region.size > 0:
    #                         # CLIPìœ¼ë¡œ í•´ë‹¹ ì˜ì—­ ë¶„ë¥˜
    #                         pil_crop = Image.fromarray(cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB))
    #                         crop_tensor = self.clip_preprocess(pil_crop).unsqueeze(0).to(self.device)
                            
    #                         text_query = f"a photo of {target_label.label_name}"
    #                         text_token = clip.tokenize([text_query]).to(self.device)
                            
    #                         with torch.no_grad():
    #                             # íŠ¹ì§• ì¶”ì¶œ
    #                             crop_features = self.clip_model.encode_image(crop_tensor)
    #                             text_features = self.clip_model.encode_text(text_token)
                                
    #                             # L2 ì •ê·œí™” (ì¤‘ìš”!)
    #                             crop_features = crop_features / crop_features.norm(dim=-1, keepdim=True)
    #                             text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                                
    #                             # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì´ì œ -1ì—ì„œ 1 ì‚¬ì´)
    #                             similarity = (crop_features @ text_features.T).cpu().numpy()[0][0]
                                
    #                             # 0-1 ë²”ìœ„ë¡œ ë³€í™˜ (ì„ íƒì )
    #                             similarity_normalized = (similarity + 1) / 2
                                
    #                             print(f"     CLIP ìœ ì‚¬ë„: ì›ë³¸={similarity:.3f}, ì •ê·œí™”={similarity_normalized:.3f}")
                            
    #                         # CLIP ì„ê³„ê°’ (ì •ê·œí™”ëœ ê°’ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •)
    #                         if similarity_normalized > 0.6:  # ì‹ ë¢°ë„.
    #                             # YOLO ì‹ ë¢°ë„ì™€ CLIP ìœ ì‚¬ë„ì˜ í‰ê·  ì‚¬ìš©
    #                             combined_confidence = (float(yolo_conf) + float(similarity_normalized)) / 2
                                
    #                             detected_boxes.append({
    #                                 'box': [x1, y1, x2, y2],
    #                                 'confidence': combined_confidence,  # ê²°í•©ëœ ì‹ ë¢°ë„
    #                                 'yolo_confidence': float(yolo_conf),
    #                                 'clip_similarity': float(similarity_normalized)
    #                             })
    #                             print(f"     âœ… Box{idx}: ë§¤ì¹­! (YOLO={yolo_conf:.2f}, CLIP={similarity_normalized:.2f}, ê²°í•©={combined_confidence:.2f})")
                    
    #                 # í•´ë‹¹ ë¼ë²¨ë¡œ ë¶„ë¥˜ëœ ê°ì²´ê°€ ìˆë‹¤ë©´ íƒì§€ ê²°ê³¼ì— ì¶”ê°€
    #                 if detected_boxes:
    #                     # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
    #                     avg_confidence = sum(box['confidence'] for box in detected_boxes) / len(detected_boxes)
                        
    #                     detections.append({
    #                         'label': target_label,
    #                         'confidence': float(avg_confidence),  # 0-1 ë²”ìœ„
    #                         'count': len(detected_boxes),
    #                         'has_alert': target_label.has_alert,
    #                         'boxes': detected_boxes
    #                     })
                        
    #                     print(f"     ğŸ¯ ìµœì¢… íƒì§€: {len(detected_boxes)}ê°œ (í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.1%})")
            
    #     except Exception as e:
    #         print(f"âŒ ê°ì²´ íƒì§€ ì˜¤ë¥˜: {e}")
    #         import traceback
    #         traceback.print_exc()
        
    #     return detections

    def _process_detection(self, camera, frame, detection, target_labels):
        """íƒì§€ ê²°ê³¼ ì²˜ë¦¬ - ë°”ìš´ë”© ë°•ìŠ¤ í¬í•¨ ìŠ¤í¬ë¦°ìƒ·"""
        from .models import DetectionLog
        
        try:
            if detection['has_alert']:
                print(f"\nğŸ“ íƒì§€ ê²°ê³¼ ì²˜ë¦¬:")
                print(f"  - ì¹´ë©”ë¼: {camera.name}")
                print(f"  - ê°ì²´: {detection['label'].display_name}")
                print(f"  - ê°œìˆ˜: {detection['count']}")
                print(f"  - ì‹ ë¢°ë„: {detection['confidence']:.3f}")
                print(f"  - ì•Œë¦¼ ì—¬ë¶€: {'ì˜ˆ' if detection['has_alert'] else 'ì•„ë‹ˆì˜¤'}")
            
            # í†µí•©ëœ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ (ëª¨ë“  íƒì§€ì— ëŒ€í•´ has_alert êµ¬ë¶„í•˜ì—¬ ì €ì¥)
            screenshot_path = self._save_all_detection_screenshot(camera, frame, detection)
            
            # has_alertì¸ ê²½ìš° ì¶”ê°€ë¡œ ê¸°ì¡´ ìŠ¤í¬ë¦°ìƒ· í´ë”ì—ë„ ì €ì¥ (í˜¸í™˜ì„± ìœ ì§€)
            if detection['has_alert']:
                annotated_frame = self._draw_detection_boxes(frame, detection)
                additional_screenshot = self._save_screenshot_with_boxes(camera, annotated_frame, detection)
                
                if screenshot_path:
                    print(f"  - ğŸ“¸ í†µí•© ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
                if additional_screenshot:
                    print(f"  - ğŸ“¸ í˜¸í™˜ì„± ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {additional_screenshot}")
                
                # DBì—ëŠ” ê¸°ì¡´ ìŠ¤í¬ë¦°ìƒ· ê²½ë¡œ ì €ì¥ (í˜¸í™˜ì„±)
                screenshot_path = additional_screenshot or screenshot_path
            
            # íƒì§€ ë¡œê·¸ ì €ì¥
            log = DetectionLog.objects.create(
                camera=camera,
                camera_name=camera.name,
                camera_location=camera.location,
                detected_object=detection['label'].display_name,
                object_count=detection['count'],
                confidence=detection['confidence'],
                has_alert=detection['has_alert'],
                screenshot_path=screenshot_path
            )
            
            print(f"  - ğŸ’¾ DB ë¡œê·¸ ì €ì¥ ì™„ë£Œ (ID: {log.id})")
            
            # ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ (has_alertì¸ ê²½ìš°)
            if detection['has_alert']:
                self._send_realtime_alert(log)
                print(f"  - ğŸ“¢ ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ íƒì§€ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    def _draw_detection_boxes(self, frame, detection):
        """í”„ë ˆì„ì— ë°”ìš´ë”© ë°•ìŠ¤ì™€ ë¼ë²¨ ê·¸ë¦¬ê¸° (í•œê¸€ ì§€ì›)"""
        # í”„ë ˆì„ ë³µì‚¬ (ì›ë³¸ ë³´ì¡´)
        annotated_frame = frame.copy()
        
        # OpenCV BGRì„ PIL RGBë¡œ ë³€í™˜
        frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)
        draw = ImageDraw.Draw(pil_image)
        
        # í°íŠ¸ ì„¤ì • (í•œê¸€ ì§€ì›)
        try:
            if self.korean_font:
                font = ImageFont.truetype(self.korean_font, 24)
                small_font = ImageFont.truetype(self.korean_font, 18)
            else:
                # ê¸°ë³¸ í°íŠ¸ (ì˜ë¬¸ë§Œ)
                font = ImageFont.load_default()
                small_font = font
        except:
            font = ImageFont.load_default()
            small_font = font
        
        # ìƒ‰ìƒ ì„¤ì •
        if detection['has_alert']:
            box_color = (255, 0, 0)  # ë¹¨ê°„ìƒ‰ (ê²½ê³ )
            text_bg_color = (255, 0, 0)
        else:
            box_color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ (ì¼ë°˜)
            text_bg_color = (0, 255, 0)
        text_color = (255, 255, 255)  # í°ìƒ‰ í…ìŠ¤íŠ¸
        
        # ê° ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        if 'boxes' in detection and detection['boxes']:
            for idx, box_info in enumerate(detection['boxes']):
                x1, y1, x2, y2 = box_info['box']
                confidence = box_info['confidence']
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë‘ê»˜ 3)
                draw.rectangle([x1, y1, x2, y2], outline=box_color, width=3)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸ ì¤€ë¹„
                label_text = f"{detection['label'].display_name} {confidence*100:.1f}%"
                
                # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
                try:
                    bbox = draw.textbbox((x1, y1), label_text, font=font)
                    text_width = bbox[2] - bbox[0]
                    text_height = bbox[3] - bbox[1]
                except:
                    # textbboxê°€ ì—†ëŠ” ê²½ìš° (êµ¬ë²„ì „ Pillow)
                    text_width, text_height = draw.textsize(label_text, font=font)
                
                # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
                text_bg_y1 = max(0, y1 - text_height - 10)
                text_bg_y2 = y1
                draw.rectangle(
                    [x1, text_bg_y1, x1 + text_width + 10, text_bg_y2],
                    fill=text_bg_color
                )
                
                # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
                draw.text(
                    (x1 + 5, text_bg_y1 + 2),
                    label_text,
                    font=font,
                    fill=text_color
                )
                
                # ë°•ìŠ¤ ë²ˆí˜¸ (ì—¬ëŸ¬ ê°œì¼ ê²½ìš°)
                if len(detection['boxes']) > 1:
                    box_num_text = f"#{idx+1}"
                    draw.text(
                        (x1 + 5, y1 + 5),
                        box_num_text,
                        font=small_font,
                        fill=box_color
                    )
        
        # ì „ì²´ ì •ë³´ í‘œì‹œ (ìš°ì¸¡ ìƒë‹¨)
        info_text = f"ì´ {detection['count']}ê°œ íƒì§€"
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ì •ë³´ ë°°ê²½
        info_bg_x1 = pil_image.width - 250
        info_bg_y1 = 10
        info_bg_x2 = pil_image.width - 10
        info_bg_y2 = 70
        
        draw.rectangle(
            [info_bg_x1, info_bg_y1, info_bg_x2, info_bg_y2],
            fill=(0, 0, 0, 180)  # ë°˜íˆ¬ëª… ê²€ì •
        )
        
        draw.text(
            (info_bg_x1 + 10, info_bg_y1 + 5),
            info_text,
            font=font,
            fill=(255, 255, 255)
        )
        
        draw.text(
            (info_bg_x1 + 10, info_bg_y1 + 30),
            timestamp,
            font=small_font,
            fill=(200, 200, 200)
        )
        
        # PIL ì´ë¯¸ì§€ë¥¼ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        annotated_frame = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
        return annotated_frame
    
    def _save_screenshot_with_boxes(self, camera, annotated_frame, detection):
        """ë°”ìš´ë”© ë°•ìŠ¤ê°€ ê·¸ë ¤ì§„ ìŠ¤í¬ë¦°ìƒ· ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # íŒŒì¼ëª…ì— í•œê¸€ í¬í•¨ ì‹œ ë¬¸ì œ ë°©ì§€
            safe_object_name = detection['label'].display_name.replace(' ', '_')
            # í•œê¸€ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ID ì‚¬ìš©
            if not safe_object_name.isascii():
                safe_object_name = f"object_{detection['label'].id}"
            
            filename = f"{camera.id}_{timestamp}_{safe_object_name}.jpg"
            filepath = os.path.join(self.screenshot_dir, filename)
            
            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ (JPEG í’ˆì§ˆ 95)
            cv2.imwrite(filepath, annotated_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            print(f"    ğŸ’¾ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì™„ë£Œ: {filename}")
            return filepath
            
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def _save_all_detection_screenshot(self, camera, frame, detection):
        """ëª¨ë“  íƒì§€ ê²°ê³¼ì— ëŒ€í•œ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ (has_alertë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì €ì¥)"""
        try:
            # ë‚ ì§œë³„ í´ë” ìƒì„±
            today = datetime.now().strftime("%Y%m%d")
            
            # has_alertì— ë”°ë¼ ìƒìœ„ í´ë” êµ¬ë¶„
            if detection['has_alert']:
                base_dir = os.path.join(self.all_detection_dir, "alerts", today)
                folder_type = "ê²½ê³ íƒì§€"
            else:
                base_dir = os.path.join(self.all_detection_dir, "normal", today)
                folder_type = "ì¼ë°˜íƒì§€"
            
            if not os.path.exists(base_dir):
                os.makedirs(base_dir, exist_ok=True)
            
            # ì¹´ë©”ë¼ë³„ í´ë” ìƒì„± (ì´ë¦„ê³¼ ìœ„ì¹˜ í¬í•¨)
            camera_name_safe = camera.name.replace(' ', '_')
            camera_location_safe = camera.location.replace(' ', '_') if camera.location else "ì•Œìˆ˜ì—†ëŠ”ìœ„ì¹˜"
            camera_dir = os.path.join(base_dir, f"{camera_name_safe}_{camera_location_safe}")
            if not os.path.exists(camera_dir):
                os.makedirs(camera_dir, exist_ok=True)
            
            # íƒì§€ëœ ê°ì²´ë³„ í´ë” ìƒì„±
            display_name = detection['label'].display_name or detection['label'].label_name
            safe_object_name = display_name.replace(' ', '_')
            
            # í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° ì˜ë¬¸ ì„¤ëª… ì¶”ê°€
            if not safe_object_name.isascii():
                safe_object_name = f"{safe_object_name}_object{detection['label'].id}"
            
            object_dir = os.path.join(camera_dir, safe_object_name)
            if not os.path.exists(object_dir):
                os.makedirs(object_dir, exist_ok=True)
            
            # ë°”ìš´ë”© ë°•ìŠ¤ê°€ ê·¸ë ¤ì§„ í”„ë ˆì„ ìƒì„±
            annotated_frame = self._draw_detection_boxes(frame, detection)
            
            # íŒŒì¼ëª… ìƒì„± (ì‹œê°„ + ê°œìˆ˜ + ì‹ ë¢°ë„)
            now = datetime.now()
            timestamp = now.strftime("%Hì‹œ%Më¶„%Sì´ˆ")
            confidence = detection['confidence']
            count = detection['count']
            filename = f"{timestamp}_{count}ê°œ_ì‹ ë¢°ë„{confidence:.2f}.jpg"
            filepath = os.path.join(object_dir, filename)
            
            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            cv2.imwrite(filepath, annotated_frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            
            print(f"    ğŸ“ {folder_type} ì €ì¥: {today}/{camera_name_safe}_{camera_location_safe}/{safe_object_name}/{filename}")
            return filepath
            
        except Exception as e:
            print(f"âŒ íƒì§€ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def _save_screenshot(self, camera, frame, detection):
        """ê¸°ì¡´ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ í•¨ìˆ˜ (ë°”ìš´ë”© ë°•ìŠ¤ í¬í•¨ ë²„ì „ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)"""
        # ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê·¸ë¦° í”„ë ˆì„ ìƒì„±
        annotated_frame = self._draw_detection_boxes(frame, detection)
        # ì €ì¥
        return self._save_screenshot_with_boxes(camera, annotated_frame, detection)
    
    def _send_realtime_alert(self, log):
        """ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ - ê°œì„ ëœ ë²„ì „"""
        try:
            alert_data = {
                'type': 'detection_alert',
                'id': log.id,
                'camera_name': log.camera_name,
                'camera_location': log.camera_location,
                'detected_object': log.detected_object,
                'object_count': log.object_count,
                'detected_at': log.detected_at.isoformat(),
                'has_screenshot': bool(log.screenshot_path),
                'confidence': log.confidence,
                'is_new': True  # ìƒˆ ì•Œë¦¼ í”Œë˜ê·¸
            }
            
            # ì „ì—­ íì— ì¶”ê°€
            try:
                # íê°€ ê°€ë“ ì°¬ ê²½ìš° ì˜¤ë˜ëœ í•­ëª© ì œê±°
                if self.alert_queue.full():
                    try:
                        self.alert_queue.get_nowait()
                    except queue.Empty:
                        pass
                
                self.alert_queue.put_nowait(alert_data)
                print(f"  âœ… ì•Œë¦¼ íì— ì¶”ê°€ ì„±ê³µ: {alert_data['detected_object']}")
                print(f"  ğŸ“Š í í¬ê¸°: {self.alert_queue.qsize()}/{self.alert_queue.maxsize}")
                
                # ëª¨ë“  SSE ë¦¬ìŠ¤ë„ˆì—ê²Œ ì¦‰ì‹œ ì•Œë¦¼ (ì„ íƒì )
                for listener in ALERT_LISTENERS:
                    try:
                        listener(alert_data)
                    except:
                        pass
                        
            except queue.Full:
                print(f"  âš ï¸ ì•Œë¦¼ íê°€ ê°€ë“ ì°¸")
            except Exception as e:
                print(f"  âŒ í ì¶”ê°€ ì˜¤ë¥˜: {e}")
                
        except Exception as e:
            print(f"âŒ ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    def get_alert_queue(self):
        """ì•Œë¦¼ í ë°˜í™˜"""
        return self.alert_queue
    
    def start_all_detections(self):
        """ëª¨ë“  í™œì„± ì¹´ë©”ë¼ì— ëŒ€í•œ íƒì§€ ì‹œì‘ (DBì—ì„œ ì‹¤ì‹œê°„ ì¡°íšŒ)"""
        from .models import Camera
        
        cameras = Camera.objects.prefetch_related('target_labels').all()  # ë§¤ë²ˆ ìµœì‹  ì¹´ë©”ë¼ì™€ ë¼ë²¨ ëª©ë¡ì„ ê°€ì ¸ì˜´
        started_count = 0
        
        print(f"ğŸ”„ AI íƒì§€ ì‹œì‘: ì´ {cameras.count()}ê°œ ì¹´ë©”ë¼ í™•ì¸")
        
        for camera in cameras:
            # íƒ€ê²Ÿ ë¼ë²¨ì´ ìˆëŠ” ì¹´ë©”ë¼ë§Œ íƒì§€ ì‹œì‘
            if camera.target_labels.exists():
                self.start_detection_for_camera(camera)
                started_count += 1
                print(f"âœ… ì¹´ë©”ë¼ '{camera.name}' AI íƒì§€ ì‹œì‘ (íƒ€ê²Ÿ ë¼ë²¨: {camera.target_labels.count()}ê°œ)")
            else:
                print(f"âš ï¸ ì¹´ë©”ë¼ '{camera.name}'ì— íƒ€ê²Ÿ ë¼ë²¨ì´ ì—†ì–´ AI íƒì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
        
        print(f"ğŸ¤– ì´ {started_count}ê°œ ì¹´ë©”ë¼ì—ì„œ AI íƒì§€ ì‹œì‘ë¨")
    
    def refresh_cameras(self):
        """ì¹´ë©”ë¼ ëª©ë¡ ë³€ê²½ ê°ì§€ í›„ ìŠ¤íŠ¸ë¦¬ë°ê³¼ íƒì§€ë¥¼ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (ì•ˆì „í•œ ë²„ì „)"""
        try:
            from .models import Camera
            
            print("ğŸ”„ AI íƒì§€ ì‹œìŠ¤í…œ ì¹´ë©”ë¼ ëª©ë¡ ì—…ë°ì´íŠ¸")
            
            # í˜„ì¬ DBì˜ ì¹´ë©”ë¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            current_cameras = Camera.objects.prefetch_related('target_labels').all()
            current_camera_ids = set(camera.id for camera in current_cameras)
            
            # í˜„ì¬ í™œì„±í™”ëœ AI íƒì§€ ëª©ë¡ (ì¹´ë©”ë¼ ID ê¸°ì¤€)
            active_detections = set(self.detection_active.keys()) if self.detection_active else set()
            
            # 1. ì‚­ì œëœ ì¹´ë©”ë¼ë“¤ì˜ AI íƒì§€ ì¤‘ì§€ (ì•ˆì „í•˜ê²Œ)
            removed_camera_ids = active_detections - current_camera_ids
            for camera_id in removed_camera_ids:
                try:
                    print(f"â¹ï¸ ì‚­ì œëœ ì¹´ë©”ë¼ AI íƒì§€ ì¤‘ì§€: {camera_id}")
                    self.stop_detection_for_camera(camera_id)
                except Exception as e:
                    print(f"âš ï¸ AI íƒì§€ ì¤‘ì§€ ì˜¤ë¥˜: {e}")
            
            # 2. ìƒˆë¡œ ì¶”ê°€ë˜ê±°ë‚˜ ìˆ˜ì •ëœ ì¹´ë©”ë¼ë“¤ì˜ AI íƒì§€ ì‹œì‘/ì¤‘ì§€
            for camera in current_cameras:
                try:
                    has_labels = camera.target_labels.exists()
                    is_detecting = camera.id in active_detections
                    
                    # AI íƒì§€ ì‹œì‘ (íƒ€ê²Ÿ ë¼ë²¨ì´ ìˆê³  ì•„ì§ ì‹œì‘ë˜ì§€ ì•Šì€ ê²½ìš°)
                    if has_labels and not is_detecting:
                        print(f"ğŸ¯ ìƒˆ ì¹´ë©”ë¼ AI íƒì§€ ì‹œì‘: {camera.name} ({camera.target_labels.count()}ê°œ ë¼ë²¨)")
                        try:
                            self.start_detection_for_camera(camera)
                        except Exception as e:
                            print(f"âŒ AI íƒì§€ ì‹œì‘ ì‹¤íŒ¨: {e}")
                    
                    # íƒ€ê²Ÿ ë¼ë²¨ì´ ì—†ì–´ì§„ ê²½ìš° AI íƒì§€ ì¤‘ì§€
                    elif not has_labels and is_detecting:
                        print(f"â¹ï¸ íƒ€ê²Ÿ ë¼ë²¨ ì—†ìŒ - AI íƒì§€ ì¤‘ì§€: {camera.name}")
                        try:
                            self.stop_detection_for_camera(camera.id)
                        except Exception as e:
                            print(f"âš ï¸ AI íƒì§€ ì¤‘ì§€ ì˜¤ë¥˜: {e}")
                            
                except Exception as e:
                    print(f"âš ï¸ ì¹´ë©”ë¼ '{camera.name}' ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            print("âœ… AI íƒì§€ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ AI íƒì§€ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    def stop_all_detections(self):
        """ëª¨ë“  íƒì§€ ì¤‘ì§€"""
        for camera_id in list(self.detection_active.keys()):
            self.stop_detection_for_camera(camera_id)

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
camera_streamer = CameraStreamer()
ai_detection_system = AIDetectionSystem()

# ì•Œë¦¼ í ì ‘ê·¼ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def get_global_alert_queue():
    """ì „ì—­ ì•Œë¦¼ í ë°˜í™˜"""
    return GLOBAL_ALERT_QUEUE