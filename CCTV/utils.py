# CCTV/utils.py
import cv2
import threading
import time
import queue
import os
from django.http import StreamingHttpResponse
from django.conf import settings
import json
import numpy as np
from collections import deque
from ultralytics import YOLO
import torch
from PIL import Image
import clip
from datetime import datetime

class CameraStreamer:
    def __init__(self):
        self.cameras = {}
        self.global_lock = threading.Lock()
        self.active_streams = {}
        self.frame_queues = {}
        self.reader_threads = {}
    
    def get_camera_stream(self, rtsp_url):
        with self.global_lock:
            if rtsp_url not in self.cameras:
                self.cameras[rtsp_url] = {
                    'cap': None,
                    'is_connected': False,
                    'last_frame': None,
                    'fps_counter': 0,
                    'last_fps_time': time.time(),
                    'avg_fps': 0,
                    'tracker_count': 0,
                    'lock': threading.Lock(),
                    'stream_count': 0,
                    'reconnect_attempts': 0,
                    'last_reconnect_time': 0
                }
                # ê° ì¹´ë©”ë¼ë³„ í”„ë ˆì„ í ìƒì„± (ìµœëŒ€ 2ê°œ í”„ë ˆì„ë§Œ ë³´ê´€)
                self.frame_queues[rtsp_url] = queue.Queue(maxsize=2)
            return self.cameras[rtsp_url]
    
    def connect_camera(self, rtsp_url):
        camera_info = self.get_camera_stream(rtsp_url)
        
        with camera_info['lock']:
            # ì¬ì—°ê²° ì‹œë„ ì œí•œ
            current_time = time.time()
            if camera_info['reconnect_attempts'] >= 3:
                if current_time - camera_info['last_reconnect_time'] < 30:
                    return False
                else:
                    camera_info['reconnect_attempts'] = 0
            
            if camera_info['cap'] is None or not camera_info['is_connected']:
                try:
                    if camera_info['cap']:
                        camera_info['cap'].release()
                        time.sleep(0.1)
                    
                    # OpenCV ì„¤ì • ìµœì í™”
                    cap = cv2.VideoCapture(rtsp_url, cv2.CAP_FFMPEG)
                    
                    # RTSP ìŠ¤íŠ¸ë¦¼ ìµœì í™” ì„¤ì •
                    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ë²„í¼ í¬ê¸° ìµœì†Œí™”
                    cap.set(cv2.CAP_PROP_FPS, 25)
                    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'H264'))
                    
                    # FFMPEG ì˜µì…˜ ì„¤ì •
                    cap.set(cv2.CAP_PROP_OPEN_TIMEOUT_MSEC, 5000)
                    cap.set(cv2.CAP_PROP_READ_TIMEOUT_MSEC, 5000)
                    
                    if cap.isOpened():
                        # ì²« í”„ë ˆì„ í…ŒìŠ¤íŠ¸
                        ret, test_frame = cap.read()
                        if ret and test_frame is not None:
                            camera_info['cap'] = cap
                            camera_info['is_connected'] = True
                            camera_info['reconnect_attempts'] = 0
                            
                            # í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ ì‹œì‘
                            if rtsp_url not in self.reader_threads or not self.reader_threads[rtsp_url].is_alive():
                                reader_thread = threading.Thread(
                                    target=self._frame_reader_thread,
                                    args=(rtsp_url,),
                                    daemon=True
                                )
                                self.reader_threads[rtsp_url] = reader_thread
                                reader_thread.start()
                            
                            return True
                        else:
                            cap.release()
                            camera_info['reconnect_attempts'] += 1
                            camera_info['last_reconnect_time'] = current_time
                            return False
                    else:
                        camera_info['reconnect_attempts'] += 1
                        camera_info['last_reconnect_time'] = current_time
                        return False
                except Exception as e:
                    print(f"Camera connection error: {e}")
                    if 'cap' in locals() and cap:
                        cap.release()
                    camera_info['reconnect_attempts'] += 1
                    camera_info['last_reconnect_time'] = current_time
                    return False
            return camera_info['is_connected']
    
    def _frame_reader_thread(self, rtsp_url):
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í”„ë ˆì„ì„ ì§€ì†ì ìœ¼ë¡œ ì½ì–´ì„œ íì— ì €ì¥"""
        camera_info = self.cameras.get(rtsp_url)
        frame_queue = self.frame_queues.get(rtsp_url)
        
        if not camera_info or not frame_queue:
            return
        
        consecutive_failures = 0
        
        while True:
            with camera_info['lock']:
                cap = camera_info['cap']
                if not cap or not camera_info['is_connected']:
                    break
                stream_count = camera_info['stream_count']
            
            if stream_count <= 0:
                time.sleep(0.1)
                continue
            
            try:
                ret, frame = cap.read()
                
                if ret and frame is not None:
                    consecutive_failures = 0
                    
                    # ì´ì „ í”„ë ˆì„ ì œê±° (íê°€ ê°€ë“ ì°¬ ê²½ìš°)
                    try:
                        frame_queue.get_nowait()
                    except queue.Empty:
                        pass
                    
                    # ìƒˆ í”„ë ˆì„ ì¶”ê°€
                    try:
                        frame_queue.put_nowait(frame)
                    except queue.Full:
                        pass
                    
                    # FPS ê³„ì‚°
                    with camera_info['lock']:
                        camera_info['fps_counter'] += 1
                        current_time = time.time()
                        if current_time - camera_info['last_fps_time'] >= 1.0:
                            camera_info['avg_fps'] = camera_info['fps_counter']
                            camera_info['fps_counter'] = 0
                            camera_info['last_fps_time'] = current_time
                else:
                    consecutive_failures += 1
                    if consecutive_failures > 10:
                        with camera_info['lock']:
                            camera_info['is_connected'] = False
                            if camera_info['cap']:
                                camera_info['cap'].release()
                                camera_info['cap'] = None
                        break
                
                # CPU ë¶€í•˜ ê°ì†Œë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.001)
                
            except Exception as e:
                print(f"Frame reading error: {e}")
                consecutive_failures += 1
                if consecutive_failures > 10:
                    with camera_info['lock']:
                        camera_info['is_connected'] = False
                        if camera_info['cap']:
                            camera_info['cap'].release()
                            camera_info['cap'] = None
                    break
    
    def generate_frames(self, rtsp_url):
        camera_info = self.get_camera_stream(rtsp_url)
        frame_queue = self.frame_queues.get(rtsp_url)
        
        with camera_info['lock']:
            camera_info['stream_count'] += 1
        
        last_frame = None
        error_count = 0
        
        try:
            while True:
                if not self.connect_camera(rtsp_url):
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + 
                           self.get_error_frame("Camera Disconnected") + b'\r\n')
                    time.sleep(2)
                    continue
                
                try:
                    # íì—ì„œ ìµœì‹  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                    frame = frame_queue.get(timeout=0.5)
                    last_frame = frame
                    error_count = 0
                except queue.Empty:
                    error_count += 1
                    if error_count > 10:
                        # 10íšŒ ì´ìƒ í”„ë ˆì„ì„ ëª» ë°›ìœ¼ë©´ ì—°ê²° ë¬¸ì œë¡œ íŒë‹¨
                        with camera_info['lock']:
                            camera_info['is_connected'] = False
                        yield (b'--frame\r\n'
                               b'Content-Type: image/jpeg\r\n\r\n' + 
                               self.get_error_frame("No Signal") + b'\r\n')
                        continue
                    elif last_frame is not None:
                        # ë§ˆì§€ë§‰ í”„ë ˆì„ ì¬ì‚¬ìš©
                        frame = last_frame
                    else:
                        continue
                
                # JPEG ì¸ì½”ë”© (í’ˆì§ˆ ì¡°ì •ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ê°ì†Œ)
                encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 70]
                _, buffer = cv2.imencode('.jpg', frame, encode_param)
                frame_bytes = buffer.tobytes()
                
                yield (b'--frame\r\n'
                       b'Content-Type: image/jpeg\r\n'
                       b'Content-Length: ' + f'{len(frame_bytes)}'.encode() + b'\r\n\r\n' + 
                       frame_bytes + b'\r\n')
                
                # í”„ë ˆì„ ë ˆì´íŠ¸ ì œì–´ (25 FPS)
                time.sleep(0.04)
                
        except GeneratorExit:
            pass
        finally:
            with camera_info['lock']:
                camera_info['stream_count'] -= 1
                if camera_info['stream_count'] <= 0:
                    # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ì´ ì¢…ë£Œë˜ë©´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                    if camera_info['cap']:
                        camera_info['cap'].release()
                        camera_info['cap'] = None
                    camera_info['is_connected'] = False
                    
                    # í”„ë ˆì„ í ë¹„ìš°ê¸°
                    try:
                        while not frame_queue.empty():
                            frame_queue.get_nowait()
                    except:
                        pass
    
    def get_error_frame(self, message="Camera Error"):
        """ì—ëŸ¬ ë©”ì‹œì§€ê°€ í¬í•¨ëœ í”„ë ˆì„ ìƒì„±"""
        error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
        
        # ë°°ê²½ìƒ‰ ì„¤ì • (ì–´ë‘ìš´ íšŒìƒ‰)
        error_frame.fill(30)
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1
        thickness = 2
        text_size = cv2.getTextSize(message, font, font_scale, thickness)[0]
        
        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚° (ì¤‘ì•™)
        text_x = (error_frame.shape[1] - text_size[0]) // 2
        text_y = (error_frame.shape[0] + text_size[1]) // 2
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        cv2.putText(error_frame, message, (text_x, text_y), 
                   font, font_scale, (0, 0, 255), thickness)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        cv2.putText(error_frame, timestamp, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        _, buffer = cv2.imencode('.jpg', error_frame)
        return buffer.tobytes()
    
    def get_camera_status(self, rtsp_url):
        camera_info = self.get_camera_stream(rtsp_url)
        with camera_info['lock']:
            return {
                'is_connected': camera_info['is_connected'],
                'avg_fps': camera_info['avg_fps'],
                'tracker_count': camera_info['tracker_count'],
                'stream_count': camera_info['stream_count'],
                'reconnect_attempts': camera_info['reconnect_attempts']
            }
    
    def cleanup_camera(self, rtsp_url):
        """ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        with self.global_lock:
            if rtsp_url in self.cameras:
                camera_info = self.cameras[rtsp_url]
                with camera_info['lock']:
                    if camera_info['cap']:
                        camera_info['cap'].release()
                        camera_info['cap'] = None
                    camera_info['is_connected'] = False
                
                # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
                if rtsp_url in self.reader_threads:
                    thread = self.reader_threads[rtsp_url]
                    if thread.is_alive():
                        thread.join(timeout=2.0)
                    del self.reader_threads[rtsp_url]
                
                # í ì •ë¦¬
                if rtsp_url in self.frame_queues:
                    del self.frame_queues[rtsp_url]
                
                del self.cameras[rtsp_url]

class AIDetectionSystem:
    def __init__(self):
        self.yolo_model = None
        self.clip_model = None
        self.clip_preprocess = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.detection_threads = {}
        self.detection_active = {}
        self.screenshot_dir = os.path.join(settings.MEDIA_ROOT, 'screenshots')
        self.load_models()
        self.ensure_screenshot_dir()
    
    def ensure_screenshot_dir(self):
        """ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.screenshot_dir):
            os.makedirs(self.screenshot_dir, exist_ok=True)
    
    def load_models(self):
        """YOLO11 ë° CLIP ëª¨ë¸ ë¡œë“œ (ë””ë²„ê·¸ ì¶”ê°€)"""
        print("\nğŸ”§ AI ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
        print(f"  - PyTorch ë²„ì „: {torch.__version__}")
        print(f"  - CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"  - CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}")
        
        try:
            # ë¦¬ëˆ…ìŠ¤/ìš°ë¶„íˆ¬ í™˜ê²½ì—ì„œ ë””ìŠ¤í”Œë ˆì´ ì„œë²„ ì—†ì´ OpenCV ì‹¤í–‰ ì„¤ì •
            import platform
            if platform.system() == 'Linux':
                os.environ['DISPLAY'] = ':0'  # X11 ë””ìŠ¤í”Œë ˆì´ ì„¤ì •
                # í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì—ì„œ GUI ë°±ì—”ë“œ ë¹„í™œì„±í™”
                import matplotlib
                matplotlib.use('Agg')
            
            # YOLO11 ëª¨ë¸ ë¡œë“œ
            yolo_path = os.path.join(settings.BASE_DIR, 'CCTV', 'yolo11n.pt')
            print(f"  - YOLO ëª¨ë¸ ê²½ë¡œ: {yolo_path}")
            print(f"  - YOLO ëª¨ë¸ ì¡´ì¬: {os.path.exists(yolo_path)}")
            
            if os.path.exists(yolo_path):
                # í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì—ì„œ YOLO ëª¨ë¸ ë¡œë“œ ì‹œ verbose=False ì„¤ì •
                self.yolo_model = YOLO(yolo_path)
                # GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° CPUë¡œ ê°•ì œ ì„¤ì •
                if not torch.cuda.is_available():
                    self.device = "cpu"
                print(f"âœ… YOLO11 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {yolo_path} (device: {self.device})")
                
                # YOLO í´ë˜ìŠ¤ ì •ë³´ ì¶œë ¥
                if hasattr(self.yolo_model, 'model') and hasattr(self.yolo_model.model, 'names'):
                    print(f"  - YOLO í´ë˜ìŠ¤ ìˆ˜: {len(self.yolo_model.model.names)}")
                    print(f"  - YOLO ì£¼ìš” í´ë˜ìŠ¤: {list(self.yolo_model.model.names.values())[:10]}...")
            else:
                print(f"âŒ YOLO11 ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {yolo_path}")
            
            # CLIP ëª¨ë¸ ë¡œë“œ
            try:
                print(f"\n  - CLIP ëª¨ë¸ ë¡œë“œ ì¤‘...")
                self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
                print(f"âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {self.device})")
            except Exception as clip_error:
                # CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ CPUë¡œ ì¬ì‹œë„
                print(f"âš ï¸ CLIP GPU ë¡œë“œ ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„: {clip_error}")
                self.device = "cpu"
                self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
                print(f"âœ… CLIP ëª¨ë¸ CPU ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ ì‹œìŠ¤í…œì´ ê³„ì† ë™ì‘í•˜ë„ë¡ ì„¤ì •
            self.yolo_model = None
            self.clip_model = None
    
    def start_detection_for_camera(self, camera):
        """íŠ¹ì • ì¹´ë©”ë¼ì— ëŒ€í•œ íƒì§€ ì‹œì‘"""
        if camera.id not in self.detection_active:
            self.detection_active[camera.id] = True
            detection_thread = threading.Thread(
                target=self._detection_worker,
                args=(camera,),
                daemon=True
            )
            self.detection_threads[camera.id] = detection_thread
            detection_thread.start()
            print(f"ğŸ¯ ì¹´ë©”ë¼ '{camera.name}' íƒì§€ ì‹œì‘")
    
    def stop_detection_for_camera(self, camera_id):
        """íŠ¹ì • ì¹´ë©”ë¼ì— ëŒ€í•œ íƒì§€ ì¤‘ì§€"""
        if camera_id in self.detection_active:
            self.detection_active[camera_id] = False
            print(f"â¹ï¸ ì¹´ë©”ë¼ ID {camera_id} íƒì§€ ì¤‘ì§€")
    
    def _detection_worker(self, camera):
        """ì¹´ë©”ë¼ë³„ íƒì§€ ì›Œì»¤ ìŠ¤ë ˆë“œ (ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€)"""
        from .models import TargetLabel, DetectionLog
        
        print(f"\nğŸš€ íƒì§€ ì›Œì»¤ ì‹œì‘: ì¹´ë©”ë¼ '{camera.name}' (ID: {camera.id})")
        
        while self.detection_active.get(camera.id, False):
            try:
                # ì¹´ë©”ë¼ì—ì„œ ìµœì‹  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                camera_info = camera_streamer.get_camera_stream(camera.rtsp_url)
                
                if not camera_info['is_connected']:
                    print(f"âš ï¸ ì¹´ë©”ë¼ '{camera.name}' ì—°ê²°ë˜ì§€ ì•ŠìŒ. ëŒ€ê¸° ì¤‘...")
                    time.sleep(2)
                    continue
                
                # í”„ë ˆì„ íì—ì„œ ìµœì‹  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                frame_queue = camera_streamer.frame_queues.get(camera.rtsp_url)
                if not frame_queue or frame_queue.empty():
                    time.sleep(0.5)
                    continue
                
                try:
                    frame = frame_queue.get_nowait()
                    print(f"\nğŸ“¹ í”„ë ˆì„ íšë“: ì¹´ë©”ë¼ '{camera.name}' - í¬ê¸°: {frame.shape}")
                except queue.Empty:
                    time.sleep(0.5)
                    continue
                
                # íƒ€ê²Ÿ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
                target_labels = list(camera.target_labels.all())
                if not target_labels:
                    print(f"âš ï¸ ì¹´ë©”ë¼ '{camera.name}'ì— íƒ€ê²Ÿ ë¼ë²¨ì´ ì—†ìŒ")
                    time.sleep(2)
                    continue
                
                print(f"ğŸ¯ íƒ€ê²Ÿ ë¼ë²¨ {len(target_labels)}ê°œ ë¡œë“œ")
                
                # ê°ì²´ íƒì§€ ìˆ˜í–‰
                detections = self._detect_objects(frame, target_labels)
                
                # íƒì§€ ê²°ê³¼ ì²˜ë¦¬
                if detections:
                    print(f"\nâœ¨ íƒì§€ ì™„ë£Œ! {len(detections)}ê°œ íƒ€ê²Ÿ ë°œê²¬")
                    for detection in detections:
                        self._process_detection(camera, frame, detection, target_labels)
                else:
                    print(f"ğŸ’¤ íƒì§€ëœ ê°ì²´ ì—†ìŒ")
                
                # íƒì§€ ê°„ê²© (1.5ì´ˆ)
                time.sleep(1.5)
                
            except Exception as e:
                print(f"âŒ íƒì§€ ì›Œì»¤ ì˜¤ë¥˜ (ì¹´ë©”ë¼: {camera.name}): {e}")
                import traceback
                traceback.print_exc()
                time.sleep(2)
        
        print(f"ğŸ›‘ íƒì§€ ì›Œì»¤ ì¢…ë£Œ: ì¹´ë©”ë¼ '{camera.name}'")
    
    def _detect_objects(self, frame, target_labels):
        """í”„ë ˆì„ì—ì„œ ê°ì²´ íƒì§€ (ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€)"""
        detections = []
        
        if self.yolo_model is None or self.clip_model is None:
            print("âš ï¸ ë””ë²„ê·¸: YOLO ë˜ëŠ” CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return detections
        
        try:
            print(f"\nğŸ” ë””ë²„ê·¸: ê°ì²´ íƒì§€ ì‹œì‘")
            print(f"  - íƒ€ê²Ÿ ë¼ë²¨: {[f'{tl.display_name}({tl.label_name})' for tl in target_labels]}")
            
            # YOLOë¡œ 1ì°¨ ê°ì²´ íƒì§€ (ë°”ìš´ë”© ë°•ìŠ¤ íšë“)
            results = self.yolo_model(frame, verbose=False)
            
            if not results or len(results) == 0:
                print("  - YOLO íƒì§€ ê²°ê³¼: ì—†ìŒ")
                return detections
            
            yolo_result = results[0]
            
            # YOLOê°€ íƒì§€í•œ ê° ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ CLIPìœ¼ë¡œ ë¶„ë¥˜
            if hasattr(yolo_result, 'boxes') and yolo_result.boxes is not None:
                boxes = yolo_result.boxes.xyxy.cpu().numpy()  # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                confidences = yolo_result.boxes.conf.cpu().numpy() if yolo_result.boxes.conf is not None else []
                classes = yolo_result.boxes.cls.cpu().numpy() if yolo_result.boxes.cls is not None else []
                
                print(f"  - YOLO íƒì§€ ìˆ˜: {len(boxes)}ê°œ")
                
                # YOLO í´ë˜ìŠ¤ ì´ë¦„ ì¶œë ¥ (ë””ë²„ê·¸ìš©)
                if len(classes) > 0:
                    class_names = yolo_result.names if hasattr(yolo_result, 'names') else {}
                    detected_classes = [class_names.get(int(cls), f'class_{int(cls)}') for cls in classes]
                    print(f"  - YOLO íƒì§€ í´ë˜ìŠ¤: {detected_classes}")
                    print(f"  - YOLO ì‹ ë¢°ë„: {[f'{conf:.2f}' for conf in confidences]}")
                
                # ì‹ ë¢°ë„ 0.5 ì´ìƒì¸ ë°”ìš´ë”© ë°•ìŠ¤ë§Œ ì‚¬ìš©
                high_conf_mask = confidences >= 0.5
                valid_boxes = boxes[high_conf_mask]
                valid_confidences = confidences[high_conf_mask]
                
                print(f"  - ì‹ ë¢°ë„ 0.5 ì´ìƒ: {len(valid_boxes)}ê°œ")
                
                if len(valid_boxes) == 0:
                    return detections
                
                # ê° íƒ€ê²Ÿ ë¼ë²¨ì— ëŒ€í•´ CLIPìœ¼ë¡œ ë¶„ë¥˜
                for target_label in target_labels:
                    print(f"\n  ğŸ“ íƒ€ê²Ÿ ë¼ë²¨ '{target_label.display_name}' ({target_label.label_name}) ê²€ì‚¬:")
                    
                    clip_count = 0
                    total_confidence = 0
                    box_details = []
                    
                    # ê° ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ì„ CLIPìœ¼ë¡œ ë¶„ë¥˜
                    for idx, (box, yolo_conf) in enumerate(zip(valid_boxes, valid_confidences)):
                        x1, y1, x2, y2 = map(int, box)
                        
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ ì¶”ì¶œ
                        cropped_region = frame[y1:y2, x1:x2]
                        
                        if cropped_region.size > 0:
                            # CLIPìœ¼ë¡œ í•´ë‹¹ ì˜ì—­ ë¶„ë¥˜
                            pil_crop = Image.fromarray(cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB))
                            crop_tensor = self.clip_preprocess(pil_crop).unsqueeze(0).to(self.device)
                            
                            # í…ìŠ¤íŠ¸ ì¿¼ë¦¬
                            text_query = f"a photo of {target_label.label_name}"
                            text_token = clip.tokenize([text_query]).to(self.device)
                            
                            with torch.no_grad():
                                crop_features = self.clip_model.encode_image(crop_tensor)
                                text_features = self.clip_model.encode_text(text_token)
                                
                                # ìœ ì‚¬ë„ ê³„ì‚°
                                similarity = (crop_features @ text_features.T).cpu().numpy()[0][0]
                                
                                box_details.append({
                                    'box_idx': idx,
                                    'coords': f"({x1},{y1})-({x2},{y2})",
                                    'yolo_conf': f"{yolo_conf:.2f}",
                                    'clip_sim': f"{similarity:.3f}"
                                })
                                
                                # CLIP ì„ê³„ê°’ (0.2 ì´ìƒì´ë©´ í•´ë‹¹ ê°ì²´ë¡œ íŒë‹¨)
                                if similarity > 0.2:
                                    clip_count += 1
                                    total_confidence += similarity
                                    print(f"     âœ… Box{idx}: CLIP ë§¤ì¹­! (ìœ ì‚¬ë„: {similarity:.3f})")
                                else:
                                    print(f"     âŒ Box{idx}: CLIP ë¯¸ë§¤ì¹­ (ìœ ì‚¬ë„: {similarity:.3f})")
                    
                    # ë°•ìŠ¤ë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥
                    print(f"     ë°•ìŠ¤ ìƒì„¸:")
                    for detail in box_details:
                        print(f"       - Box{detail['box_idx']}: {detail['coords']}, "
                            f"YOLOì‹ ë¢°ë„={detail['yolo_conf']}, CLIPìœ ì‚¬ë„={detail['clip_sim']}")
                    
                    # í•´ë‹¹ ë¼ë²¨ë¡œ ë¶„ë¥˜ëœ ê°ì²´ê°€ ìˆë‹¤ë©´ íƒì§€ ê²°ê³¼ì— ì¶”ê°€
                    if clip_count > 0:
                        avg_confidence = total_confidence / clip_count
                        
                        detections.append({
                            'label': target_label,
                            'confidence': float(avg_confidence),
                            'count': clip_count,  # CLIPìœ¼ë¡œ ì •í™•íˆ ì„¼ ê°œìˆ˜
                            'has_alert': target_label.has_alert
                        })
                        
                        print(f"     ğŸ¯ ìµœì¢… íƒì§€: {clip_count}ê°œ (í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f})")
                        print(f"     ğŸš¨ ì•Œë¦¼ ì„¤ì •: {'í™œì„±í™”' if target_label.has_alert else 'ë¹„í™œì„±í™”'}")
                    else:
                        print(f"     â­• íƒì§€ë˜ì§€ ì•ŠìŒ")
            
            print(f"\nğŸ“Š ì „ì²´ íƒì§€ ê²°ê³¼: {len(detections)}ê°œ íƒ€ê²Ÿ ë°œê²¬")
            for det in detections:
                print(f"  - {det['label'].display_name}: {det['count']}ê°œ (ì‹ ë¢°ë„: {det['confidence']:.3f})")
        
        except Exception as e:
            print(f"âŒ ê°ì²´ íƒì§€ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        return detections

    
    def _process_detection(self, camera, frame, detection, target_labels):
        """íƒì§€ ê²°ê³¼ ì²˜ë¦¬ (ë¡œê·¸ ì €ì¥, ì•Œë¦¼, ìŠ¤í¬ë¦°ìƒ·) - ë””ë²„ê·¸ ì¶”ê°€"""
        from .models import DetectionLog
        
        try:
            print(f"\nğŸ“ íƒì§€ ê²°ê³¼ ì²˜ë¦¬:")
            print(f"  - ì¹´ë©”ë¼: {camera.name}")
            print(f"  - ê°ì²´: {detection['label'].display_name}")
            print(f"  - ê°œìˆ˜: {detection['count']}")
            print(f"  - ì‹ ë¢°ë„: {detection['confidence']:.3f}")
            print(f"  - ì•Œë¦¼ ì—¬ë¶€: {'ì˜ˆ' if detection['has_alert'] else 'ì•„ë‹ˆì˜¤'}")
            
            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ (has_alertì¸ ê²½ìš°)
            screenshot_path = None
            if detection['has_alert']:
                screenshot_path = self._save_screenshot(camera, frame, detection)
                if screenshot_path:
                    print(f"  - ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
                else:
                    print(f"  - âš ï¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨")
            
            # íƒì§€ ë¡œê·¸ ì €ì¥
            log = DetectionLog.objects.create(
                camera=camera,
                camera_name=camera.name,
                camera_location=camera.location,
                detected_object=detection['label'].display_name,
                object_count=detection['count'],
                confidence=detection['confidence'],
                has_alert=detection['has_alert'],
                screenshot_path=screenshot_path
            )
            
            print(f"  - ğŸ’¾ DB ë¡œê·¸ ì €ì¥ ì™„ë£Œ (ID: {log.id})")
            
            # ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ (has_alertì¸ ê²½ìš°)
            if detection['has_alert']:
                self._send_realtime_alert(log)
                print(f"  - ğŸ“¢ ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
            
            print(f"ğŸ¯ íƒì§€ ë¡œê·¸: {log}")
            
        except Exception as e:
            print(f"âŒ íƒì§€ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    

    def _save_screenshot(self, camera, frame, detection):
        """ìŠ¤í¬ë¦°ìƒ· ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{camera.id}_{timestamp}_{detection['label'].display_name}.jpg"
            filepath = os.path.join(self.screenshot_dir, filename)
            
            cv2.imwrite(filepath, frame)
            return filepath
            
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def _send_realtime_alert(self, log):
        """ì‹¤ì‹œê°„ ì•Œë¦¼ ì „ì†¡ (SSE íì— ì¶”ê°€)"""
        # SSE ì•Œë¦¼ íì— ì¶”ê°€ (ì¶”í›„ êµ¬í˜„)
        alert_data = {
            'type': 'detection_alert',
            'camera_name': log.camera_name,
            'camera_location': log.camera_location,
            'detected_object': log.detected_object,
            'object_count': log.object_count,
            'detected_at': log.detected_at.isoformat(),
            'has_screenshot': bool(log.screenshot_path)
        }
        
        # ê¸€ë¡œë²Œ ì•Œë¦¼ íì— ì¶”ê°€ (ì¶”í›„ SSEì—ì„œ ì‚¬ìš©)
        if not hasattr(self, 'alert_queue'):
            self.alert_queue = queue.Queue()
        
        try:
            self.alert_queue.put_nowait(alert_data)
        except queue.Full:
            pass  # íê°€ ê°€ë“ ì°¬ ê²½ìš° ë¬´ì‹œ
    
    def start_all_detections(self):
        """ëª¨ë“  í™œì„± ì¹´ë©”ë¼ì— ëŒ€í•œ íƒì§€ ì‹œì‘"""
        from .models import Camera
        
        cameras = Camera.objects.all()
        for camera in cameras:
            if camera.target_labels.exists():  # íƒ€ê²Ÿ ë¼ë²¨ì´ ìˆëŠ” ì¹´ë©”ë¼ë§Œ
                self.start_detection_for_camera(camera)
    
    def stop_all_detections(self):
        """ëª¨ë“  íƒì§€ ì¤‘ì§€"""
        for camera_id in list(self.detection_active.keys()):
            self.stop_detection_for_camera(camera_id)

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
camera_streamer = CameraStreamer()
ai_detection_system = AIDetectionSystem()