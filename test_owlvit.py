import torch
import numpy as np
from PIL import Image, ImageDraw
from transformers import OwlViTProcessor, OwlViTForObjectDetection
import matplotlib.pyplot as plt

class ImprovedObjectClassifier:
    def __init__(self):
        self.load_owlvit()
        
    def load_owlvit(self):
        """OWL-ViT ëª¨ë¸ ë¡œë“œ"""
        print("Loading OWL-ViT...")
        self.processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
        self.model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")
        print("âœ… OWL-ViT loaded successfully")
    
    def classify_cropped_objects(self, image_crops, text_queries, confidence_threshold=0.3):
        """í¬ë¡­ëœ ê°ì²´ë“¤ì„ ì •í™•íˆ ë¶„ë¥˜"""
        results = []
        
        for i, crop in enumerate(image_crops):
            print(f"\nğŸ” Crop {i+1} ë¶„ì„ ì¤‘...")
            
            # OWL-ViTë¡œ ë¶„ë¥˜
            inputs = self.processor(text=text_queries, images=crop, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            target_sizes = torch.Tensor([crop.size[::-1]])
            results_per_crop = self.processor.post_process_object_detection(
                outputs=outputs, 
                target_sizes=target_sizes, 
                threshold=confidence_threshold
            )
            
            # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ íƒì§€ ê²°ê³¼ ì„ íƒ
            if len(results_per_crop[0]["scores"]) > 0:
                best_idx = torch.argmax(results_per_crop[0]["scores"])
                best_score = results_per_crop[0]["scores"][best_idx].item()
                best_label_idx = results_per_crop[0]["labels"][best_idx].item()
                best_text = text_queries[best_label_idx]
                
                # ëª¨ë“  í´ë˜ìŠ¤ì˜ ì ìˆ˜ ê³„ì‚° (ì¶”ê°€ ë¶„ì„ìš©)
                all_scores = {}
                for j, query in enumerate(text_queries):
                    mask = results_per_crop[0]["labels"] == j
                    if mask.any():
                        scores_for_class = results_per_crop[0]["scores"][mask]
                        all_scores[query] = scores_for_class.max().item()
                    else:
                        all_scores[query] = 0.0
                
                result = {
                    'crop_index': i,
                    'best_match': best_text,
                    'confidence': best_score,
                    'all_scores': all_scores,
                    'is_confident': best_score > confidence_threshold
                }
            else:
                result = {
                    'crop_index': i,
                    'best_match': 'unknown',
                    'confidence': 0.0,
                    'all_scores': {query: 0.0 for query in text_queries},
                    'is_confident': False
                }
            
            results.append(result)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"   Best match: {result['best_match']} ({result['confidence']:.4f})")
            for query, score in result['all_scores'].items():
                print(f"   '{query}': {score:.4f}")
        
        return results
    
    def improved_text_queries_generator(self, base_objects):
        """ë” êµ¬ì²´ì ì´ê³  êµ¬ë³„í•˜ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±"""
        improved_queries = {}
        
        # ê¸°ë³¸ ê°ì²´ë³„ë¡œ ë” êµ¬ì²´ì ì¸ ì„¤ëª… ì¶”ê°€
        object_descriptions = {
            'monitor': [
                'computer monitor screen',
                'desktop display screen',
                'LCD monitor with black frame',
                'computer screen showing display'
            ],
            'keyboard': [
                'computer keyboard with keys',
                'QWERTY keyboard layout',
                'keyboard with multiple keys arranged in rows',
                'typing keyboard with letter keys'
            ],
            'speaker': [
                'desktop speaker with driver cone',
                'audio speaker with round driver',
                'computer speaker with mesh grille',
                'speaker with visible driver unit'
            ],
            'mouse': [
                'computer mouse with buttons',
                'optical computer mouse',
                'desktop mouse with scroll wheel',
                'computer pointing device mouse'
            ]
        }
        
        for obj in base_objects:
            if obj in object_descriptions:
                improved_queries[obj] = object_descriptions[obj]
            else:
                improved_queries[obj] = [f'a {obj}']
        
        return improved_queries
    
    def multi_query_classification(self, image_crops, base_objects):
        """ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•œ ê°•í™”ëœ ë¶„ë¥˜"""
        improved_queries = self.improved_text_queries_generator(base_objects)
        
        final_results = []
        
        for i, crop in enumerate(image_crops):
            print(f"\nğŸ” Multi-query analysis for Crop {i+1}")
            
            object_scores = {}
            
            # ê° ê°ì²´ë³„ë¡œ ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
            for obj_name, queries in improved_queries.items():
                scores_for_object = []
                
                for query in queries:
                    inputs = self.processor(text=[query], images=crop, return_tensors="pt")
                    
                    with torch.no_grad():
                        outputs = self.model(**inputs)
                    
                    target_sizes = torch.Tensor([crop.size[::-1]])
                    results = self.processor.post_process_object_detection(
                        outputs=outputs, 
                        target_sizes=target_sizes, 
                        threshold=0.1  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘
                    )
                    
                    if len(results[0]["scores"]) > 0:
                        max_score = results[0]["scores"].max().item()
                        scores_for_object.append(max_score)
                    else:
                        scores_for_object.append(0.0)
                
                # í•´ë‹¹ ê°ì²´ì˜ í‰ê·  ì ìˆ˜
                object_scores[obj_name] = np.mean(scores_for_object) if scores_for_object else 0.0
                print(f"   {obj_name}: {object_scores[obj_name]:.4f} (queries: {len(queries)})")
            
            # ìµœê³  ì ìˆ˜ ê°ì²´ ì„ íƒ
            best_object = max(object_scores.keys(), key=lambda k: object_scores[k])
            best_score = object_scores[best_object]
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ìµœê³  ì ìˆ˜ì™€ ë‘ ë²ˆì§¸ ì ìˆ˜ì˜ ì°¨ì´)
            sorted_scores = sorted(object_scores.values(), reverse=True)
            confidence_gap = sorted_scores[0] - sorted_scores[1] if len(sorted_scores) > 1 else sorted_scores[0]
            
            result = {
                'crop_index': i,
                'best_match': best_object,
                'confidence': best_score,
                'confidence_gap': confidence_gap,
                'all_scores': object_scores,
                'is_confident': confidence_gap > 0.15  # ì°¨ì´ê°€ 0.15 ì´ìƒì´ë©´ ì‹ ë¢°í• ë§Œí•¨
            }
            
            final_results.append(result)
            
            print(f"   âœ… Final: {best_object} (score: {best_score:.4f}, gap: {confidence_gap:.4f})")
        
        return final_results
    
    def visualize_classification_results(self, image_crops, results):
        """ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™”"""
        num_crops = len(image_crops)
        fig, axes = plt.subplots(2, num_crops, figsize=(4*num_crops, 8))
        
        if num_crops == 1:
            axes = axes.reshape(2, 1)
        
        for i, (crop, result) in enumerate(zip(image_crops, results)):
            # ì›ë³¸ í¬ë¡­ ì´ë¯¸ì§€
            axes[0, i].imshow(crop)
            axes[0, i].set_title(f"Crop {i+1}")
            axes[0, i].axis('off')
            
            # ë¶„ë¥˜ ê²°ê³¼ ë°”ì°¨íŠ¸
            objects = list(result['all_scores'].keys())
            scores = list(result['all_scores'].values())
            
            colors = ['red' if obj == result['best_match'] else 'lightblue' for obj in objects]
            
            axes[1, i].bar(range(len(objects)), scores, color=colors)
            axes[1, i].set_xticks(range(len(objects)))
            axes[1, i].set_xticklabels(objects, rotation=45, ha='right')
            axes[1, i].set_ylim(0, 1)
            axes[1, i].set_title(f"Best: {result['best_match']}\n"
                               f"Conf: {result['confidence']:.3f}")
            axes[1, i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
def test_with_your_image():
    """ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ"""
    classifier = ImprovedObjectClassifier()
    
    # ì—¬ëŸ¬ë¶„ì˜ í¬ë¡­ëœ ì´ë¯¸ì§€ë“¤ì„ ë¡œë“œ
    # (YOLO ê²°ê³¼ì—ì„œ ë‚˜ì˜¨ í¬ë¡­ë“¤)
    
    # ì˜ˆì‹œ - ì‹¤ì œë¡œëŠ” YOLOì—ì„œ ë‚˜ì˜¨ í¬ë¡­ ì´ë¯¸ì§€ë“¤ì„ ì‚¬ìš©
    image_crops = [
        Image.open("clip_images/tmpeb_qopzg.PNG"),  # ëª¨ë‹ˆí„° í¬ë¡­
        # Image.open("crop_2.jpg"),  # í‚¤ë³´ë“œ í¬ë¡­
        # Image.open("crop_3.jpg"),  # ìŠ¤í”¼ì»¤ í¬ë¡­
    ]
    
    # íƒì§€í•˜ê³  ì‹¶ì€ ê°ì²´ë“¤
    target_objects = ['monitor', 'keyboard', 'speaker', 'mouse']
    
    # ê°œì„ ëœ ë¶„ë¥˜ ì‹¤í–‰
    results = classifier.multi_query_classification(image_crops, target_objects)
    
    # ê²°ê³¼ ì‹œê°í™”
    classifier.visualize_classification_results(image_crops, results)
    
    return results

if __name__ == "__main__":
    print("ğŸ¯ ê°œì„ ëœ ê°ì²´ ë¶„ë¥˜ê¸°")
    print("="*50)
    print("íŠ¹ì§•:")
    print("- OWL-ViT ê¸°ë°˜ìœ¼ë¡œ ë” ì •í™•í•œ ê°ì²´ íƒì§€")
    print("- ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ì‹ ë¢°ë„ í–¥ìƒ")
    print("- ì‹ ë¢°ë„ ê°­ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •")
    print()
    print("ì„¤ì¹˜ ëª…ë ¹:")
    print("pip install transformers torch torchvision matplotlib pillow")
    print()
    print("ì‚¬ìš©ë²•:")
    print("classifier = ImprovedObjectClassifier()")
    print("results = classifier.multi_query_classification(image_crops, target_objects)")

    test_with_your_image()